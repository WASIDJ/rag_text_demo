{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json \n",
    "import pdfplumber\n",
    "import time\n",
    "import jwt\n",
    "import requests\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:26.236414400Z",
     "start_time": "2024-05-26T09:36:25.851492Z"
    }
   },
   "id": "89b0b02a8505902c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 实际KEY，过期时间\n",
    "def generate_token(apikey: str, exp_seconds: int):#\n",
    "    try:\n",
    "        id, secret = apikey.split(\".\")# 通过.分割字符串 生成id和secret 用于加密 保证数据的安全性\n",
    "    except Exception as e:\n",
    "        raise Exception(\"invalid apikey\", e)\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": id,\n",
    "        \"exp\": int(round(time.time() * 1000)) + exp_seconds * 1000,  # 过期时间 当前时间+过期时间 1000是为了转换成毫秒\n",
    "        \"timestamp\": int(round(time.time() * 1000)),  # 时间戳 当前时间 1000是为了转换成毫秒\n",
    "    }\n",
    "    return jwt.encode(\n",
    "        payload,  # 加密的内容\n",
    "        secret,  # 加密的密钥\n",
    "        algorithm=\"HS256\",  # 加密的算法\n",
    "        headers={\"alg\": \"HS256\", \"sign_type\": \"SIGN\"},  # 加密的头部  alg:加密算法  sign_type:签名类型  typ:token类型\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:26.245011100Z",
     "start_time": "2024-05-26T09:36:26.235413300Z"
    }
   },
   "id": "110369319da1fa45"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def glm_predict( content):\n",
    "    url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': generate_token(\"885c8c10b3f5267ab2a7dd1c7f9eb75a.qUEzlKy72Y79RMdv\", 1000)\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"glm-3-turbo\",\n",
    "        \"messages\": [  #模拟多人对话 \n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    print(\"LLM Response:\")\n",
    "    print(response)\n",
    "    return response.json()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:26.258554900Z",
     "start_time": "2024-05-26T09:36:26.247009800Z"
    }
   },
   "id": "540ce735171764f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "questions = json.load(open('./大数据导论知识问答/submit.json', 'r', encoding='utf-8'))\n",
    "pdf = pdfplumber.open('./大数据导论知识问答/大数据导论.pdf')\n",
    "pdf_content = {}\n",
    "for idx in range(len(pdf.pages)):\n",
    "    pdf_content[\"page_\" + str(idx + 1)] = pdf.pages[idx].extract_text()#.extract_text()提取文本"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:38:01.601670800Z",
     "start_time": "2024-05-26T09:36:54.779592Z"
    }
   },
   "id": "832ae2e0dfa466b0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 24\n",
      "<class 'str'>\n",
      "l 第 1 章大数据概述 仁二＝＝二\t运用大数据促进保障和改善民生、切实保障国家数据安全等五项工作部署，为我国发展大数据开\t启了新的篇章。 2018 年 4 月 22 日至 24 日，首届“数字中国”建设峰会在福建省福州市举行（见\t图 1-6 )，围绕“以信息化驱动现代化，加快建设数字中国“主题，来自各省、市、区网信部门负\t责人、行业组织负责人、产业界代表、专家学者以及智库代表等约 800 人出席峰会，就建设网络\t强国、数字中国、智慧社会等热点议题进行交流分享。\t图 1-6 首届“数字中国”建设峰会\t1.5\t大数据的概念\t随着大数据时代的到来，“大数据”已经成为互联网信息技术行业的流行词汇。 关于“什么是\t大数据“这个问题，大家比较认可的是关于大数据的 “4V\" 说法。 大数据的 4 个 “V\"，或者说是\t大数据的 4 个特点，包含 4个层面：数据量大 ( Volume)、数据类型繁多 (Variety)、处理速度快\t(Velocity) 和价值密度低 (Value)。\t1.5.1 数据量大\t从数据扯的角度而言，大数据泛指无法在可容忍的时间内用传统信息技术和软 、硬件主具对\t其进行获取、管理和处理的巨量数据集合，需要可伸缩的计算体系结构以支持其存储、处理和分\t析。 按照这个标准来衡量，很显然，目前的很多应用场景中所涉及的数据批已经具备了大数据的\t特征。 比如，微博、微信、抖音等应用平台的每天由网民发布的海最信息，属于大数据，再比如，\t遍布我们飞作和生活的各个角落的各种传感器和摄像头，每时每刻都在自动产生大扯数据，这也\t屈丁大数据。\t13\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 16\n",
      "<class 'str'>\n",
      "l 第 1 章大数据概述 1\t在数据爆炸的今天，人类一方面对知识充满渴求，另一方面为数据的复杂特征所困惑。 数据\t爆炸对科学研究提出了更高的要求，人类需要设计出更加灵活高效的数据存储、处理和分析丁．具、\t来应对大数据时代的挑战，巾此，必将带来云计算、数据仓库、数据挖掘等技术和应用的提升或\t者根本性的改变。 在存储（存储技术）领域，需要实现低成本的大规模分布式存储；在网络效率\t（网络技术）方面，需要实现及时响应用户体验功能；在数据中心方面，需要开发更加绿色节能的\t新一代数据中心，在有效面对大数据处理需求的同时，实现最大化资源利用率、最小化系统能耗\t的目标，\t1.2\t大数据时代\t第三次信息化浪潮涌动，大数据时代全面到来。 人类社会信息科技的发展为大数据时代的到\t米提供了技术支撑，而数据产生方式的变革是促进大数据时代到来的至关重要的囚素。\t1.2.1 第三次信息化浪潮\t根据 IBM 公司前首席执行官郭士纳的观点， IT 领域每隔 15 年就会迎来一次重大变革（见\t表 1-1)。 1980 年前后，个人计算机 (PC) 开始普及，计算机逐渐走入企业和千家万户，大大提\t高了社会生产力，也使人类迎来了第一次信息化浪潮， Intel、 AMD、 IBM、苹果、微软、联想等\t企业是这个时期的标志。 随后，在 1995 年前后，人类开始全面进入互联网时代，互联网的普及把\t叽界变成“地球村”，每个人都可以自由邀游于信息的海洋，由此，人类迎来了第二次信息化浪潮\t这个时期也缔造 f雅虎、谷歌、阿里巴巴、百度等互联网”巨头＂。 时隔 15 年，在 2010 年前后，\t云计符、大数据、 物联网的快速发展，拉开了第三次信息化浪潮的大幕，大数据时代的到来，也\t必将涌现出一批新的市场标杆企业。\t表 1-1 三次信息化浪潮\t信息化浪潮 发生时间 标志 韶决的问题 代表企业\tIntel、 AMD、 TBM、 苹果、 微软、 联想、\t第一次信息化浪潮 1980 年前后 个人计算机 信息处理\t戴尔、惠许等\t第二次信息化浪潮 1995 年前后 互联网 信息传输 雅虎、谷歌、 阿里巴巴、 百度、腾讯等\t物联网、云计 亚马逊、谷歌、 IBM、 VMware、 Palantir、\t第＝次信息化浪潮 2010 年前后 信息爆炸\t算和大数据 Hortonw orks、 Cloudera、 阿里压等\t1.2.2 信息科技为大数据时代提供技术支撑\t大数据，首先会带来一场技术革命， 毫无疑问，如果没有强大的数据存储、传输和计算等技\t术能力，缺乏必要的设施、 设备，大数据的应用就无从谈起。 从这个意义上说｀信息科技进步是\t大数据时代的物质基础 信息科技需要韶决信息存储、信息处理和信息传输 3 个核心问题~ 人类\t5\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 19\n",
      "<class 'str'>\n",
      "=＝才 大数据导论 l\t方式大致经历了 3 个阶段： 运营式系统阶段、用户原创内容阶段和感知式系统阶段\t1. 运营式系统阶段\t人类最早大规模管理和使用数据，是从数据库的诞生开始的。 大型零售超市销售系统、银行交易\t系统、股票交易系统、医院医疗系统、企业客户管理系统等大扯运营式系统，都是建立在数据库基础\t之上的，数据库中保存了大扯结构化的企业关键信息，用来满足企业各种业务需求。 在这个阶段，数\t据的产生方式是被动的，只有当实际的企业业务发生时，才会产生新的数据并存入数据库、比如，对\t于股票交易市场而言，只有当发生一笔股票交易时，股票交易系统才会有相关数据生成。\t2. 用户原创内容阶段\t互联网的出现，使得数据传播更加快捷， 数据传播不需要借助于磁盘、磁带等物理存储介质\t网页的出现进一步加速了大批网络内容的产生，从而使得人类社会数据址开始呈现“井喷式”增\t长但是，真正的互联网数据爆发产生千以“用户原创内容”为特征的 “Web 2.0 时代”“Web 1.0\t时代”主要以门户网站为代表，强调内容的组织与提供，大批用户本身并不参与内容的产生 而\tWeb 2.0 技术以微博、微信、抖音等应用所采用的自服务模式为主，强调自服务，大批用户本身\t就是内容的生成者． 尤其是随着移动互联网和智能手机终端的普及，人们更是可以随时随地使川\t手机发微博、传照片等，数据批开始急剧增长 从此｀每个人都是海揽数据中的微小的一部分\t每天我们通过微信、 QQ 、新浪微博等各种方式采集到大撮数据，然后通过同样的渠道和方式把处\t理过的数据反馈出去 而这些数据不断地被存储和加丁＿，使得互联网世界里的“公开数据“不断\t被卡富，这大大加速了大数据时代的到来。\t3. 感知式系统阶段\t物联网的发展最终导致了人类社会数据址的第＝欴次跃升。 物联网中包含大批传感器，如温度\t传感器、湿度传感器、压力传感器、位移传感器、光电传感器等，此外，视频监控摄像头也是物\t联网的重要组成部分 物联网中的这些设备 、 每时每刻都会自动产生大扯数据（见图 1-5)，与\tWeb 2.0 时代的人丁数据产生方式相比，物联网中的自动数据产生方式，将在短时间内生成更密\t集、更大址的数据，使得人类社会迅速步入 “大数据时代”。\t图 1-5 物联网设备每时每刻都会自动产生大址数据\t8\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 22\n",
      "<class 'str'>\n",
      "l 第 1 章大数据概述仁二二二\t1.4.1 美国\t美国是率先将大数据从商业上升至国家战略的国家。 通过稳步实施“三步走”战略，美国在\t大数据技术研发、商业应用以及保障国家安全等方面已全面构筑起全球领先优势。 第一步是快速\t部署大数据核心技术研究，并在部分领域积极开发大数据应用；第二步是调整政策框架与法律规\t章，积极应对大数据发展带来的隐私保护等问题；第三步是强化数据驱动的体系和能力建设，为\t提升国家整体竞争力提供长远保障。\t2012 年 3 月美国联邦政府推出“大数据研究和发展倡议”，其中对于国家大数据战略的表述\t如下：“通过收集、处理庞大而复杂的数据信息，从中获得知识和洞见，提升能力，加快科学、工\t程领域的创新步伐，强化美国国土安全，转变教育和学习模式”。 2012 年 3 月 29 日，美国白宫科\t技政策办公室发布《大数据研究和发展计划》，成立“大数据高级指导小组＂。 该计划旨在通过对\t海量和复杂的数字资料进行收集、整理，提高美国联邦政府收集海量数据、分析萃取信息的能力\t和对社会经济发展的预测能力。 2013 年 11 月，美国信息技术与创新基金会发布了《支持数据驱\t动型创新的技术与政策》的报告，报告指出，“数据驱动型创新”是一个崭新的命题，其中最主要\t的包括“大数据““开放数据”“数据科学”“云计算\"。 2014年 5 月美国发布《大数据：把握机遇，\t守护价值》白皮书，对美国大数据应用与管理的现状、政策框架和改进建议进行了集中阐述。 该\t白皮书表示，在大数据发挥正面价值的同时，应该警惕大数据应用对隐私、公平等长远价值带来\t的负面影响。\t1.4.2 英国\t大数据发展初期，英国在借鉴美国经验和做法的基础上，充分结合本国特点和需求，加大大\t数据研发投入、强化顶层设计，聚焦部分应用领域进行重点突破。 英国政府千 2010 年上线政府数\t据网站，它同美国的政府数据网站功能类似，但侧重于大数据信息挖掘和获取能力的提升。 以此\t作为基础，英国政府在 2012 年发布了新的政府数字化战略，具体由英国商业创新技能部牵头，成\t立数据战略委员会，通过大数据开放，为政府、私人部门、第三方组织和个体提供相关服务，吸\t纳更多技术力扯和资金支持拓宽数据来源，以推动就业和新兴产业的发展，实现大数据驱动的社\t会经济增长。 2013 年英国政府加大了对大数据领域研究的资金支持，提出总额 1.89 亿英镑的资助\t计划，包括直接投资 1000 万英镑建立“开放数据研究所”。\t1.4.3 法国\t法国是传统的工业大国和经济强国，在信息化战略的推动下，法国大数据产业也逐步发展起\t来，并已经渗透到社会经济生活的多个领域，影响着人们的生活和工作，甚至影响着城市管理、\t公共管理等方面。\t2011 年 7 月，法国启动了开放数据项目，通过实现在移动终端上使用公共数据，最大限度地\t挖掘数据的应用价值。 项目内容涉及交通、文化、旅游和环境等领域。 所有法国公民和在法国旅\t11\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 123\n",
      "<class 'str'>\n",
      "＝ 大数据导论 |\t为用户提供定制化的数据服务，由于需要涉及数据的处理加工，因此，该类型平台的业务相对复\t杂，国内大数据交易平台大多属于这种类型 而第三方数据交易平台业务相对简单明确，主要负\t责对交易过程的监管，通常可以提供数据出售、数据购买、数据供应方查询以及数据需求发布等\t服务。\t此外， 从大数据交易平台的建设与运营主体角度来说，目前的大数据交易平台还可以划分为\t3 种类型： 政府主导的大数据交易平台、企业以市场需求为导向建立的大数据交易平台、产业联\t盟性质的大数据交易平台。 其中，产业联盟性质的大数据交易平台（如中关村大数据产业联盟、\t中国大数据产业联盟、上海大数据产业联盟），侧亟于数据的共享，而不是数据的交易，\t2. 交易平台的数据来源\t交易平台的数据来源主要包括政府公开数据、企业内部数据、数据供应方数据、 网页爬虫数\t据等。\t(1) 政府公开数据。 政府数据资源开放共享是世界各国实施大数据发展战略的重要举措。 政\t府作为公共数据的核心生产者和拥有者，汇集了最具挖掘价值的数据资源。 加快政府数据开放共\t享，释放政府数据和机构数据的价值．对大数据交易市场的繁荣将起到重要作用。\t(2) 企业内部数据。 企业在生产经营过程中，积累了海批的数据，包括产品数据、设备数据、\t研发数据、供应链数据、运营数据、管理数据、销售数据、消费者数据等，这些数据经过处理加\t工以后，是具有重要商业价值的数据源。\t(3) 数据供应方数据。 该类型的数据一般是由数据供应方在数据交易平台上根据交易平台的\t规则和流程提供自己所拥有的数据。\t(4) 网页爬虫数据。 通过相关技术手段，从全球范围内的互联网网页爬取的数据。\t多种数据来源渠道可以使得交易平台数据更加丰富，但是，同时增加了数据监管难度。 在 IT\t飞速发展的时代，信息收集变得更加容易，信息滥用、个人数据倒卖情况屡见不鲜，因此，在数\t据来源广泛的情况下，更要加强对交易平台的安全监管。\t3. 交易平台的产品类型\t不同的交易平台会根据自己的目标和定位， 提供不同的产品类型，用户可以根据自己的个性\t化需求合理地选择交易平台。 交易产品的类型主要有以下几种 ： API、数据包、云服务、解决方\t案、数据定制服务以及数据产品。\t( 1) APl。 API 是应用程序接口，数据供应方对外提供数据访间接口 ， 数据需求方直接通过调\t用接口来获得所需的数据。\t(2) 数据包。 数据包的数据，既可以是未经处理的原始数据，也可以是经过加工处理以后的\t数据。\t(3) 云服务。 云服务是在云计算不断发展的背景下产生的，通常通过互联网来提供实时的、\t动态的资源。\t(4) 解决方案。 在特定的情景下， 利用已有的数据，为需求方提供处理问题的方案，比如数\t据分析报告等。\t112\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 13\n",
      "<class 'str'>\n",
      "二大数据导论 l\t图 1-1 数据也被称为“未来的石油”\t1.1.2 数据类型\t常见的数据类型包括文本、图片、音频、视频等。\t(I) 文本：文本数据是指不能参与算术运算的任何字符，也称为字符型数据。 在计算机中，\t文本数据一般保存在文本文件中。 文本文件是一种由若干行字符构成的计算机文件，常见格式包\t括 ASCII、 M压庄和 TXT 等。\t(2) 图片：图片是指由图形、图像等构成的平面媒体。 在计算机中，图片数据一般用图片格\t式的文件来保存。 图片的格式很多，大体可以分为点阵图和矢址图两大类，我们常用的 BMP、 JPG\t等格式的图片属于点阵图，而 Flash 动画制作软件所生成的 SWF 等格式的文件和 Photoshop 绘图\t软件所生成的 PSD 等格式的图片属于矢蜇图。\t(3) 音频：数字化的声音数据就是音频数据。 在计算机中，音频数据一般用音频文件的格式\t来保存。 音频文件是指存储声音内容的文件，把音频文件用一定的音频程序执行，就可以还原以\t前录下的声音。 音频文件的格式很多，包括 CD、 WAV、 MP3、 MID、 WMA、 RM 等。\t(4) 视频：视频数据是指连续的图像序列。 在计算机中，视频数据一般用视频文件的格式来\t保存。 视频文件常见的格式包括 MPEG-4、 AVI、 DAT、 RM、 MOV、 ASF、 WMV、 DivX 等。\t1.1.3 数据组织形式\t计算机系统中的数据组织形式主要有两种，即文件和数据库。\t(1) 文件：计算机系统中的很多数据都是以文件形式存在的，比如一个 Word 文件、 一个文\t本文件、 一个网页文件、一个图片文件等。一个文件的文件名包含主名和扩展名，扩展名用来表\t示文件的类型，比如文本、图片、音频、视频等。 在计算机中，文件是由文件系统负责管理的。\t(2) 数据库：计算机系统中另一种非常重要的数据组织形式就是数据库。 今天，数据库已经\t成为计算机软件开发的基础和核心之一，在人力资源管理、固定资产管理、制造业管理、 电信管\t理、销售管理、 售票管理、银行管理、股市管理、教学管理、图书馆管理、政务管理等领域发挥\t2\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 19\n",
      "<class 'str'>\n",
      "=＝才 大数据导论 l\t方式大致经历了 3 个阶段： 运营式系统阶段、用户原创内容阶段和感知式系统阶段\t1. 运营式系统阶段\t人类最早大规模管理和使用数据，是从数据库的诞生开始的。 大型零售超市销售系统、银行交易\t系统、股票交易系统、医院医疗系统、企业客户管理系统等大扯运营式系统，都是建立在数据库基础\t之上的，数据库中保存了大扯结构化的企业关键信息，用来满足企业各种业务需求。 在这个阶段，数\t据的产生方式是被动的，只有当实际的企业业务发生时，才会产生新的数据并存入数据库、比如，对\t于股票交易市场而言，只有当发生一笔股票交易时，股票交易系统才会有相关数据生成。\t2. 用户原创内容阶段\t互联网的出现，使得数据传播更加快捷， 数据传播不需要借助于磁盘、磁带等物理存储介质\t网页的出现进一步加速了大批网络内容的产生，从而使得人类社会数据址开始呈现“井喷式”增\t长但是，真正的互联网数据爆发产生千以“用户原创内容”为特征的 “Web 2.0 时代”“Web 1.0\t时代”主要以门户网站为代表，强调内容的组织与提供，大批用户本身并不参与内容的产生 而\tWeb 2.0 技术以微博、微信、抖音等应用所采用的自服务模式为主，强调自服务，大批用户本身\t就是内容的生成者． 尤其是随着移动互联网和智能手机终端的普及，人们更是可以随时随地使川\t手机发微博、传照片等，数据批开始急剧增长 从此｀每个人都是海揽数据中的微小的一部分\t每天我们通过微信、 QQ 、新浪微博等各种方式采集到大撮数据，然后通过同样的渠道和方式把处\t理过的数据反馈出去 而这些数据不断地被存储和加丁＿，使得互联网世界里的“公开数据“不断\t被卡富，这大大加速了大数据时代的到来。\t3. 感知式系统阶段\t物联网的发展最终导致了人类社会数据址的第＝欴次跃升。 物联网中包含大批传感器，如温度\t传感器、湿度传感器、压力传感器、位移传感器、光电传感器等，此外，视频监控摄像头也是物\t联网的重要组成部分 物联网中的这些设备 、 每时每刻都会自动产生大扯数据（见图 1-5)，与\tWeb 2.0 时代的人丁数据产生方式相比，物联网中的自动数据产生方式，将在短时间内生成更密\t集、更大址的数据，使得人类社会迅速步入 “大数据时代”。\t图 1-5 物联网设备每时每刻都会自动产生大址数据\t8\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 28\n",
      "<class 'str'>\n",
      "l 第 1 章大数据概述 仁二二二二\t实验科学、理论科学、计算科学和数据密集型科学四种范式（见图］－8), 具体如下。\t•+ C\t,\t.\tB\t·t\t_\t__ ,\t一， ＿\t实验f1学 埋论ft学 il·rt-H学 数拟密优刑干1学\t图 1-8 科学研究的四种范式\t1．第一种范式：实验科学\t在最初的科学研究阶段，人类采用实验来解决一些科学问题，著名的比萨斜塔实验就是一个\t典型实例。 1590 年，伽利略在比萨斜塔上做了“两个铁球同时落地”的实验，得出了亟趾不同的\t两个铁球同时下落的结论，从此推翻了亚里士多德”物体下落速度和重拭成比例＂的学说，纠正\tf这个持续了 1900 年之久的错误结论。\t2. 第二种范式：理论科学\t实验科学的研究会受到当时实验条件的限制，难以完成对自然现象更精确的理解。 随若科学\t的进步，人类开始采用数学、几何、物理等理论，构建问题模型和寻找解决方案。 比如牛顿第一\t定律、牛顿第二定律、牛顿第气定律构成了牛顿经典力学的体系， 奠定了经典力学的概念基础，\t它的广泛传播和运用对人们的生活和思想产生了重大影响，在很大程度上推动了人类社会的发展。\t3. 第三种范式：计算科学\t1946 年，随着人类历史上第一台通用计算机 ENIAC 的诞生，人类社会步入计算机时代，科\t学研究也进入了一个以“计算”为中心的全新时期。 在实际应用中，计算科学主要用于对各个科\t学问题进行计算机模拟和其他形式的计算。 通过设计算法并编写相应程序输入计算机运行，人类\t可以借助计算机的高速运莽能力去解决各种问题。 计算机具有存储容批大、运算速度快、精度高、\t可重复执行等特点，是科学研究的利器，推动了人类社会的飞速发展。\t4. 第四种范式：数据密集型科学\t随着数据的不断累积，其宝贵价值日益得到体现，物联网和云计算的出现，更促成了事物发\t展从批到质的转变，使人类社会开启了全新的大数据时代。 如今，计算机不仅能做模拟仿真，还\t能进行分析总结，得到理论。 在大数据环境下，一切将以数据为中心，从数据中发现问题、解决\t问题，真正体现数据的价值。 大数据成为科学工作者的宝藏，从数据中可以挖掘未知模式和有价\t值的信息，服务于生产和生活，推动科技创新和社会进步。 虽然第三种范式和第四种范式都是利\t川计弈机来进行计算，但是， 二者还是有本质的区别。 在第＝．种范式中，一般是先提出可能的理\t17\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 16\n",
      "<class 'str'>\n",
      "l 第 1 章大数据概述 1\t在数据爆炸的今天，人类一方面对知识充满渴求，另一方面为数据的复杂特征所困惑。 数据\t爆炸对科学研究提出了更高的要求，人类需要设计出更加灵活高效的数据存储、处理和分析丁．具、\t来应对大数据时代的挑战，巾此，必将带来云计算、数据仓库、数据挖掘等技术和应用的提升或\t者根本性的改变。 在存储（存储技术）领域，需要实现低成本的大规模分布式存储；在网络效率\t（网络技术）方面，需要实现及时响应用户体验功能；在数据中心方面，需要开发更加绿色节能的\t新一代数据中心，在有效面对大数据处理需求的同时，实现最大化资源利用率、最小化系统能耗\t的目标，\t1.2\t大数据时代\t第三次信息化浪潮涌动，大数据时代全面到来。 人类社会信息科技的发展为大数据时代的到\t米提供了技术支撑，而数据产生方式的变革是促进大数据时代到来的至关重要的囚素。\t1.2.1 第三次信息化浪潮\t根据 IBM 公司前首席执行官郭士纳的观点， IT 领域每隔 15 年就会迎来一次重大变革（见\t表 1-1)。 1980 年前后，个人计算机 (PC) 开始普及，计算机逐渐走入企业和千家万户，大大提\t高了社会生产力，也使人类迎来了第一次信息化浪潮， Intel、 AMD、 IBM、苹果、微软、联想等\t企业是这个时期的标志。 随后，在 1995 年前后，人类开始全面进入互联网时代，互联网的普及把\t叽界变成“地球村”，每个人都可以自由邀游于信息的海洋，由此，人类迎来了第二次信息化浪潮\t这个时期也缔造 f雅虎、谷歌、阿里巴巴、百度等互联网”巨头＂。 时隔 15 年，在 2010 年前后，\t云计符、大数据、 物联网的快速发展，拉开了第三次信息化浪潮的大幕，大数据时代的到来，也\t必将涌现出一批新的市场标杆企业。\t表 1-1 三次信息化浪潮\t信息化浪潮 发生时间 标志 韶决的问题 代表企业\tIntel、 AMD、 TBM、 苹果、 微软、 联想、\t第一次信息化浪潮 1980 年前后 个人计算机 信息处理\t戴尔、惠许等\t第二次信息化浪潮 1995 年前后 互联网 信息传输 雅虎、谷歌、 阿里巴巴、 百度、腾讯等\t物联网、云计 亚马逊、谷歌、 IBM、 VMware、 Palantir、\t第＝次信息化浪潮 2010 年前后 信息爆炸\t算和大数据 Hortonw orks、 Cloudera、 阿里压等\t1.2.2 信息科技为大数据时代提供技术支撑\t大数据，首先会带来一场技术革命， 毫无疑问，如果没有强大的数据存储、传输和计算等技\t术能力，缺乏必要的设施、 设备，大数据的应用就无从谈起。 从这个意义上说｀信息科技进步是\t大数据时代的物质基础 信息科技需要韶决信息存储、信息处理和信息传输 3 个核心问题~ 人类\t5\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 203\n",
      "<class 'str'>\n",
      "＝ 大数据辱涵］\t据库提供的服务，如图 6-7 所示。 客户端不需要了解云数据库的底层细节，所有的底层硬件都巳\t经被虚拟化，对客户端而言是透明的客户端就像在使用一个运行在单一服务器上的数据库一样，\t非常方便、容易．同时可以获得理论上近乎无限的存储和处理能力。\t图 6-7 云数据库示意图\t需要指出的是．有人认为数据库屈于应用基础设施（即中间件），因此把云数据库列入 PaaS\t的范畴，也有人认为数据库本身也是一种应用软件，因此把云数据库划入 SaaS。 对千这个问题，\t本书把云数据库划入 SaaS, 但同时认为，云数据库到底应该被划入 PaaS 还是 SaaS, 这并不是最\t重要的 实际上，云计算 IaaS 、 PaaS 和 SaaS 这 3 个层次之间的界限有些时候也不是非常清晰\t对于云数据库而言，最重要的是它允许用户以服务的方式通过网络获得云端的数据。\t6.6.2 云数据库的特性\t云数据库具有以下特性。\t(l) 动态可扩展理论七，云数据库具有无限可扩展性，可以满足不断增加的数据存储需求\t在面对不断变化的条件时， 云数据库可以表现出很好的弹性。 例如，对于一个从事产品零售的电\t子商务公司，会存在季节性或突发性的产品需求变化，或者对于类似 Animoto 的网络社区站点，\t可能会经历一个指数级的用户增长阶段r 这时，它们就可以分配额外的数据库存储资源来处理增\t加的需求，这个过程只需要几分钟 一旦需求过去以后，它们就可以立即释放这些资源\t(2) 高可用性。 云数据库不存在单点失效问题。 如果一个节点失效了，剩余的节点就会接管\t未完成的事务而且，在云数据库中，数据通常是冗余存储的．在地理上也是分散的。诸如 Google、\tAmazon 和田M 等大型云计符供应商，具有分布在世界范闱内的数据中心，通过在不同地理区间\t内进行数据复制，可以提供高水平的容错能力。 例如， Amazon SimpleDB 会在不同的区域内进行\t数据复制，这样，即使某个区域内的云设施失效，也可以保证数据继续可用。\t(3) 较低的使用代价 云数据库厂商通常采用多租户 (Multi-tenancy) 的形式．同时为多个\t用户提供服务，这种共享资源的形式对于用户而言可以节省开销，而且用户采用”按需付费”的\t192\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 204\n",
      "<class 'str'>\n",
      "l 第 6 章数据存储与管理 仁二二二二\t方式使用云计算环境中的各种软、硬件资源，不会产生不必要的资源浪费。 另外，云数据库底层\t存储通常采用大量廉价的商业服务器，这大大降低了开销。 腾讯云数据库官方公布的资料显示，\t当实现类似的数据库性能时，如果采用自己投资自建 MySQL 的方式，则成本为每台服务器每天\t50.6 元，实现双机容灾需要 2 台服务器，即成本为 101.2 元，平均存储成本是每吉字节每天 0.25\t元，平均 1 元可获得的每秒查询率 (Query Per Second, QPS) 为 24 次／秒；而如果采用腾讯云数\t据库产品，企业不需要投入任何初期建设成本，成本仅为 72 元／天，平均存储成本为每吉字节每\t天 0.18 元，平均 1 元可获得的 QPS 为 83 次电从相对千自建，云数据库平均 1 元获得的 QPS 提\t高为原来的 346%，具有极高的性价比。\t(4) 易用距使用云数据库的用户不必控制运行原始数据库的机器，也不必了解它身在何处。\t用户只需要一个有效的连接字符串 (URL) 就可以开始使用云数据库，而且就像使用本地数据库\t一样。 许多基于 MySQL 的云数据库产品（如腾讯云数据库、阿里云 RDS 等），完全兼容 MySQL\t协议，用户可通过基于 MySQL 协议的客户端或者 API 访问实例。 用户可无缝地将原有 MySQL\t应用迁移到云存储平台，无需进行任何代码改造。\t(5) 高性能。 云数据库采用大型分布式存储服务集群，支撑海批数据访问，多机房自动冗余\t备份，自动读写分离。\t(6) 免维护。 用户不需要关注后端机器及数据库的稳定性、 网络问题、机房灾难、单库压力\t等各种风险，云数据库服务商提供 “7x24h\" 的专业服务，扩容和迁移对用户透明且不影响服务，\t并且可以提供全方位、全天候立体式监控，用户无需半夜去处理数据库故障。\t(7) 安全。 云数据库提供数据隔离，不同应用的数据会存在于不同的数据库中而不会相互影\t响；提供安全性检查，可以及时发现并拒绝恶意攻击性访问；提供数据多点备份，确保不会发生\t数据丢失。\t6.6.3 云数据库与其他数据库的关系\t关系数据库采用关系数据模型， NoSQL 数据库采用非关系数据模型， 二者属千不同的数据库\t技术。 从数据模型的角度来说，云数据库并非一种全新的数据库技术，而是以服务的方式提供数\t据库功能的技术。 云数据库并没有专属于自己的数据模型， 云数据库所采用的数据模型可以是关\t系数据库所使用的关系模型（如微软的 SQLAzure 云数据库、阿里云 RDS 都采用了关系模型），\t也可以是NoSQL数据库所使用的非关系模型（如 Amazon Dynamo 等云数据库采用的是＂键值”\t存储）。 同一个公司也可能提供采用不同数据模型的多种云数据库服务，例如百度云数据库提供了\t3 种数据库服务，即分布式关系数据库服务（基于关系数据库 MySQL)、 分布式非关系数据库服\t务（ 基于文档数据库 MongoDB)、 键值型非关系数据库服务（ 基千键值数据库 Redis)。实际上，\t许多公司在开发云数据库时，后端数据库都是直接使用现有的各种关系数据库或 NoSQL 数据库\t产品。 比如，腾讯云数据库采用 MySQL 作为后端数据库，微软的 SQL Azure 云数据库采用 SQL\tServer 作为后端数据库。 从市场的整体应用情况来看，由于 NoSQL 应用对开发者要求较高，而\tMySQL 拥有成熟的中间件、 运维工具，已经形成一个良性的生态系统等特性，因此从现阶段来\t193\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 44\n",
      "<class 'str'>\n",
      "1 第 2 章 大数据与其他新兴技术的关系仁二二二二\t传统的 IT 资源获取方式的主要缺点和挖井取水的缺点基本一样，具体如下。\t(I) 初期成本高，周期长。 以 100MB 的磁盘空间为例，以前，在云计算诞生之前，节一个\t企业需要获得 100MB 的磁盘空间时，需要建机房、买设备、聘请 IT 员工维护 这种做法本质上\t和“为了喝水而去挖一口井”是一样的，不仅需要投入较高的成本，还需要经过一段时间的购买、\t安装和调试设备后才能使用。\t(2) 后期需要自己维护，使用成本高。 机房的服务器发生故障、软件发生错误等问题，都需\t要企业自己去解决 为此，企业还需要为维护机房的 IT 员丁支付费用。\t(3) [T 资源供应最有限。 企业的机房建设完成后，配置的 IT 资源是固定的，比如，配置了\t1000MB 的磁盘空间，那么，每天只能最多用到 1000MB、如果要使用更多的磁盘空间，就需要\t额外购买、安装和调试。\t云计算的主要优点，和自来水的优点基本一样，具体如下 ，\t(l) 初期零成本，瞬时可获得。 当用户需要 100MB 的磁盘空间时，不需要去自建机房、买\t设备，只要连接到“云端”，就可以瞬时获得 100MB 的磁盘空间。\t(2) 后期免维护，使用成本低。 用户只需要使用云计符服务商提供的 IT 资源服务，不需要负\t贞云计算设施的维护，比如数据中心设施更换、系统维护升级、软件更新等，这些都是云计算服\t务商负责的工作，和用户无关。 而且，与投入十几万元建设机房相比，云计算的使用价格极其低\t廉． 采用”按拭计费”的方式收取费用，比如， 1GB 的磁盘空间每年收取 2 元， 2GB 的磁盘空间\t符年收取 4 元。\t(3) 在供应 IT 资源址方面”予取予求＂。 只要用户交得起租金，想要使用多少 IT 资源，阿里\t巴巴、百度 、腾讯等云计算服务商都可以为用户持续提供。\t2.1.2 云计算的服务模式和类型\t云计算包括 3 种典型的服务模式（见图 2-4)，即基础设施即服务(Infrastructure as a Service,\tlaaS)、平台即服务 (Platform as a Service, PaaS) 和软件即服务 (Software as a Service, SaaS)。\tIaaS 将基础设施（计算资源和存储设备）作为服务出租， PaaS 把平台作为服务出租． SaaS 把软\t件作为服务出租。\t_\t`r - - ^～勹一 三， 匕\t云计算包括公有云、私有云和混合云 3 j I ^ I\t, 公有云 < 混合云 、 • 私有云 吻\t~~\t种类型（见图 2-4)。 公有云面向所有用户提 、L l气…4\t供服务，只要是注册付费的用户都可以使用，\t比如 AWS; 私有云只为特定用户提供服务，\t应用层\t软件即服务 (SaaS)\t比如大型企业出千安全考虑自建的云计算环\t平台层\t境，只为企业内部提供服务；混合云综合了\t平台即服务 (PaaS)\t公有云和私有云的特点，因为，对于一些企\t基础设施层\t业而言，一方面出于安全考虑需要把数据放 基础设施即服务 (IaaS)\t在私有云中，另一方面希望可以获得公有云 图 2-4 云计算的服务模式和类型\t33\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 76\n",
      "<class 'str'>\n",
      "l 第 2 章 大数据与其他新兴技术的关系 仁二二二二\t2. 请阐述云计算有哪几种服务模式和哪几种类型。\t3. 请阐述什么是数据中心和数据中心在云计算中的作用。\t4. 请举例说明云计算有哪些典型的应用。\t5. 请阐述物联网的概念和物联网各个层次的功能。\t6. 请阐述物联网有哪些关键技术。\t7. 请阐述大数据与云计算、物联网的相互关系。\t8. 请阐述人工智能的概念。\t9. 请阐述人工智能有哪些关键技术。\t10. 请阐述人工智能与大数据的关系。\t11. 请阐述区块链的概念以及区块链和比特币的关系。\t12. 请阐述区块链是如何解决防篡改问题的。\t13. 请阐述区块链和大数据的关系。\t65\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 215\n",
      "<class 'str'>\n",
      "二二才 大数据导论|\t售趋势做出预测并做出针对性的营销改变。\t(4) 关联规则。 关联规则是隐藏在数据项之间的关联或相互关系，即可以根据一个数据项的\t出现推导出其他数据项的出现。 关联规则挖掘技术已经被广泛应用千金融行业的企业中来预测客\t户的需求，各银行在自己的 ATM 上通过捆绑客户可能感兴趣的信息供用户了解，并获取相应信\t息来改善自身的营销策略。\t(5) 协同过滤。 简单来说协同过滤就是利用兴趣相投、拥有共同经验的群体的喜好，来推荐\t用户感兴趣的信息，个人通过合作的机制给予信息相当程度的回应（如评分）并记录下来，以达\t到过滤的目的，进而帮助别人筛选信息。\t7.2.2 分类\t分类是一种重要的机器学习和数据挖掘技术。 分类的目的是根据数据集的特点构造一个分类\t函数或分类模型（也常称作分类器），该模型能把未知类别的样本映射到给定类别中。\t分类的具体规则可描述如下：给定一组训练数据的集合 T, T 的每一条记录包含由若干个属\t=\t性组成的一个特征向扯，记录用矢量 X (x1,x2 …,xn)表示。 x，可以有不同的值域，当一属性的值\t域为连续域时，该属性为连续属性 (NumericalAttribute)，否则为离散属性 (DiscreteAttribute)。\t用 C=cl,c2,...，ck 表示类别属性，即数据集有 K个不同的类别。 那么， T就隐含了一个从矢批X到类\t别属性 C 的映射函数： f(X)H C 。 分类的目的就是分析输入数据，通过在训练集中的数据表现出\t来的特性，为每一个类找到一种准确的描述或者模型，采用该种方法（模型）将隐含函数表示出来。\t构造分类模型的过程一般分为训练和测试两个阶段。 在构造模型之前，将数据集随机地分为\t训练数据集和测试数据集。 先使用训练数据集来构造分类模型，然后使用测试数据集来评估模型\t的分类准确率。 如果认为模型的准确率可以接受，就可以用该模型对其他数据元组进行分类。 一\t般来说，测试阶段的代价远低于训练阶段。\t典型的分类方法包括决策树、朴素贝叶斯、支持向量机和人工神经网络等。\t这里给出一个分类的应用实例。假设有一名植物学爱好者对她发现的鸾尾花的品种很感兴趣。\t她收集了每朵鸾尾花的一些测量数据：花瓣的长度和宽度以及花萼的长度和宽度。 她还有一些药\t尾花分类的数据，也就是说，这些花之前已经被植物学专家鉴定为属于setosa、versicolor或virginica\t3 个品种之一。 基于这些分类数据，她可以确定每朵莺尾花所属的品种。 千是，她可以构建一个\t分类算法，让算法从这些已知品种的药尾花测簸数据中进行学习，得到一个分类模型，再使用分\t类模型预测新发现的鸾尾花的品种。\t7.2.3 聚类\t聚类又称群分析， 是一种重要的机器学习和数据挖掘技术。 聚类的目的是将数据集中的数据\t对象划分到若干个簇中，并且保证每个簇之间样本尽扯接近，不同簇的样本间距离尽扯远。 通过\t聚类生成的簇是一组数据对象的集合，簇满足以下两个条件。\t(1) 每个簇至少包含一个数据对象。\t204\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 55\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t自然语言处理的应用包罗万象，例如机器翻译、手写体和印刷体字符识别、语音识别、信息\t检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数\t据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。\t4. 人机交互\t人机交互是一门研究系统与用户之间的交互关系的学科。 系统可以是各种各样的机器，也可\t以是计算机化的系统和软件。 人机交互界面通常是指用户可见的部分。 用户通过人机交互界面与\t系统进行交流和操作。 人机交互是与认知心理学、 人机工程学、多媒体技术、虚拟现实技术等密\t切相关的综合学科。 传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、\t鼠标、操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，和打印机、\t绘图仪、显示器、头盔式显示器、音箱等输出设备。 人机交互技术除了传统的基本交互和图形交\t互外，还包括语音交互、情感交互、体感交互及脑机交互等。\t人机交互具有广泛的应用场景，比如，日本建成了一栋可应用“人机交互”技术的住宅（见\t图 2-15)，人们可以通过该装置，用意念不用手就能自由操控家用电器。 该住宅主要是为身体有\t残疾的人、老年人等创造便捷的生活环境而设计的。 用户头部戴着的是含有“人机交互”技术的\t特殊装置，该装置通过读取用户脑部血流的变化和脑波变动数据实现无线通信。 连接网络的计算\t机通过识别装置发来的无线信号向机器传输指令。 目前此装置判断的准确率达 70%~80%，且从\t人的意识出现开始，最短 6.5s 内机器就可进行识别。\t图 2-15 日本建成了一栋可应用“人机交互”技术的住宅\t5. 计算机视觉\t计算机视觉是一门研究如何使机器学会“看”的科学。 更进一步地说，计算机视觉是指用摄\t影机和计算机代替人眼对目标进行识别、跟踪和测量，并进一步做图形处理，使其成为更适合人\t眼观察或传送给仪器检测的图像。 依靠计算机视觉技术自动识别室内物体和人如图 2-16 所示。 计\t算机视觉既是工程领域也是科学领域中的一个富有挑战性的重要研究领域。 计算机视觉是一门综\t44\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 74\n",
      "<class 'str'>\n",
      "l 第 2 章大数据与其他新兴技术的关系仁二＝＝二\t等风险。 用区块链技术实现资产数字化后，所有资产交易记录公开、透明、永久存储、可追溯，\t完全符合监管需求。\t(9) 社交。 区块链应用于社交领域的核心价值：让用户自己控制数据，杜绝隐私泄露。 区块\t链技术在社交领域的应用目的，就是让社交网络的控制权，从中心化的公司转向个人，实现“中\t心化＇向“去中心化”的转变，让数据的控制权牢牢掌握在用户自己手里。\t2.5.5 大数据与区块链的关系\t区块链和大数据都是新一代信息技术，二者既有区别，又存在着紧密的联系。\t1 ． 大数据与区块链的区别\t大数据与区块链的区别主要表现在以下几个方面。\t(l) 数据批。 区块链技术是分布式数据存储、点对点传输、共识机制、加密算法等计算机技\t术的新型应用模式。 区块链处理的数据扯小，具有细致的处理方式。 而大数据管理的是海量数据，\t要求广度和数量，处理方式上会更粗糙。\t(2) 结构化和非结构化。 区块链是结构定义严谨的块，通过指针组成的链，是典型的结构化\t数据，而大数据需要处理的更多的是非结构化数据。\t(3) 独立和整合。 区块链系统为保证安全性，信息是相对独立的，而大数据的重点是信息的\t整合分析。\t(4) 直接和间接。 区块链是一个分布式账本，本质上就是一个数据库，而大数据指的是对数\t据深度分析和挖掘，是一种间接的数据。\t(5) CAP 理论。 C (Consistency) 是一致性，它是指任何一个读操作总是能够读到之前完成\t的写操作的结果，也就是在分布式环境中，多点的数据是一致的。 A (Availab血y) 是可用性，它\t是指快速获取数据，可以在确定的时间内返回操作结果。 P (Tolerance of Network Partition) 是分\t区容忍性，它是指当出现网络分区的清况时（即系统中的一部分节点无法和其他节点进行通信），\t分离的系统也能够正常运行。 CAP 理论告诉我们，一个分布式系统不可能同时满足一致性、可用\t性和分区容忍性这 3 个需求，最多只能同时满足其中 2 个，正所谓“鱼和熊掌不可兼得“。 大数据\t通常选择实现 AP, 区块链则选择实现 CP。\t(6) 基础网络。 大数据底层的基础设施通常是计算机集群，而区块链的基础设施通常是 P2P\t网络。\t( 7) 价值来源。 对于大数据而言，数据是信息，需要从数据中提炼得到价值。 而对于区块链\t而言，数据是资产，是价值的传承。\t(8) 计算模式。 在大数据的场景中，是把一件事情分给多个人做，比如，在 MapReduce 计算\t框架中， 一个大型任务会被分解成很多个子任务，分配给很多个节点同时去计算。 而在区块链的\t场景中，是让多个人重复做一件事情，比如， P2P 网络中的很多个节点同时记录一笔交易。\t2. 大数据与区块链的联系\t区块链的可信任性、安全性和不可篡改性，正在让更多数据被释放出来，区块链会对大数据\t63\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 55\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t自然语言处理的应用包罗万象，例如机器翻译、手写体和印刷体字符识别、语音识别、信息\t检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数\t据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。\t4. 人机交互\t人机交互是一门研究系统与用户之间的交互关系的学科。 系统可以是各种各样的机器，也可\t以是计算机化的系统和软件。 人机交互界面通常是指用户可见的部分。 用户通过人机交互界面与\t系统进行交流和操作。 人机交互是与认知心理学、 人机工程学、多媒体技术、虚拟现实技术等密\t切相关的综合学科。 传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、\t鼠标、操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，和打印机、\t绘图仪、显示器、头盔式显示器、音箱等输出设备。 人机交互技术除了传统的基本交互和图形交\t互外，还包括语音交互、情感交互、体感交互及脑机交互等。\t人机交互具有广泛的应用场景，比如，日本建成了一栋可应用“人机交互”技术的住宅（见\t图 2-15)，人们可以通过该装置，用意念不用手就能自由操控家用电器。 该住宅主要是为身体有\t残疾的人、老年人等创造便捷的生活环境而设计的。 用户头部戴着的是含有“人机交互”技术的\t特殊装置，该装置通过读取用户脑部血流的变化和脑波变动数据实现无线通信。 连接网络的计算\t机通过识别装置发来的无线信号向机器传输指令。 目前此装置判断的准确率达 70%~80%，且从\t人的意识出现开始，最短 6.5s 内机器就可进行识别。\t图 2-15 日本建成了一栋可应用“人机交互”技术的住宅\t5. 计算机视觉\t计算机视觉是一门研究如何使机器学会“看”的科学。 更进一步地说，计算机视觉是指用摄\t影机和计算机代替人眼对目标进行识别、跟踪和测量，并进一步做图形处理，使其成为更适合人\t眼观察或传送给仪器检测的图像。 依靠计算机视觉技术自动识别室内物体和人如图 2-16 所示。 计\t算机视觉既是工程领域也是科学领域中的一个富有挑战性的重要研究领域。 计算机视觉是一门综\t44\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 65\n",
      "<class 'str'>\n",
      "＝＝才大数据导论 I\t越来越丰富。\t2.4.5 大数据与人工智能的关系\t大数据与人工智能都是当前的热门技术，人工智能的发展要早于大数据，人工智能在 20 世纪\t50 年代左右就已经开始发展，而大数据的概念直到 2010 年左右才形成。 从百度搜索指数可以看\t出（见图 2-26)，人工智能受到长期、广泛的关注，且其被关注的时间远早于大数据，在近两年\t再次被推向顶峰。 人工智能的影响力要大千大数据。 大数据从 2013 年开始得到较多关注， 2017\t年 4 月达到顶峰。\t.~.\t漫j1一 ,IO:1飞9l 凶“心心｀ .;10 ..．比t. 王JI.\t．飞云 ·人已丘 勹心尤-丑亡l匕-.e\t图 2-26 人T一智能和大数据的百度搜索指数\t人工智能和大数据是紧密相关的两种技术，二者既有联系，又有区别。\t1. 人工智能与大数据的联系\t一方面，人工智能需要数据来建立其智能，特别是机器学习。 例如，机器学习图像识别应用\t程序需要查看数以万计的飞机图像，以了解飞机的构成，以便将来能够识别出它们。 人工智能应\t用的数据越多，其获得的结果就越准确。 过去，由于处理器速度慢、数据批小，人工智能不能很\t好地工作。 今天，大数据为人工智能提供了海量的数据，使得人工智能技术有了长足的发展，甚\t至可以说，没有大数据就没有人工智能。\t另一方面，大数据技术为人工智能提供了强大的存储能力和计算能力。 在过去，人工智能算\t法都依赖于单机的存储和单机的算法。 而在大数据时代．面对海拔的数据，传统的单机存储和单\t机算法都巳经无能为力。\t2. 人工智能与大数据的区别\t人工智能与大数据存在着明显的区别，人工智能是一种计算形式，它允许计算机执行认知功\t能，例如，对输入起作用或做出反应，类似于人类的做法。 而大数据是一种传统计算，它不会根\t据结果采取行动，只是寻找结果。\t另外，二者要达成的目标和实现目标的手段不同。 大数据的主要目的是通过数据的对比分析\t来掌握和推演出更优的方案。 就拿视频推送为例，我们之所以会接收到不同的推送内容，是因为\t54\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 71\n",
      "<class 'str'>\n",
      "一 大数据导论 I\t钥对本次交易进行签名，任何人都可以通过验证函数 Verify(）来验证此次签名是否由持有张三私钥\t的张三本人发出（而不是其他人冒用张兰的名义），若是就返回 True, 否则返回 False。 Sign(）和\tVerify(）由密码学保证不被破韶。\tl签名函数Sign（张二的私钥， 转账信息： 张二转10元给李四）＝本次转账签名\tI验证函数Verify（张二的地址， 转账信息： 张分专10元给李四，本次转账签名）＝ True|\t图 2-31 签名函数和验证函数\t签名函数的执行都是自动的，并不需要我们手动去处理。 比如我们安装了比特币钱包 App,\t它就会帮我们去做这样的事情，因为钱包 App 知道我们的私钥，所以，我们只要告诉这个 App,\t我想转 10 元给李四，那么这个钱包 App 会帮我们自动生成这次转账的信息和签名，然后向全网\t发布，等待其他人使用 Verify(）函数来验证。\t4. 比特币要解决的第二个问题：去中心化记账\t一条交易记录的真实性得到确认以后，接下来的问题是，由谁负责记账呢？也就是说，巾谁\t负责把这条交易添加到区块链中呢？\t首先，我们会想到由银行、政府或支付宝这些机构负责记账，也就是采用“中心化方式”来\t记账。 然而，历史上所有巾中心化机构记账的加密数字货币尝试，都失败了。 因为中心化记账的\t缺点很多，主要如下。\t(1) 拒绝服务攻击。 对于一些特定的地址，记账机构拒绝为之提供记录服务。\t(2) 厌倦后停止服务。 如果记账机构没有从记账中获得收益，时间长了，就会停止服务”\t(3) 中心机构易被攻击。 比如服务器遭到破坏和网络攻击等。\t正是因为中心化记账会存在很多问题，因此，比特币需要解决第二个问题：去中心化\t在比特币区块链中，为了实现去中心化，采用的方式是：人人都可以记账，每个人都可以保\t留完整账本。 任何人都可以下载开源程序，加入 P2P 网络，监听全世界发送的交易，成为记账节\t点参与记账。 当 P2P 网络中的某个节点接收到一条交易记录时，它会传播给相邻的节点，然后相\t邻的节点再传播给其他相邻的节点，那么通过这样一个 P2P 网络，这个数据会瞬间传遍全球。\t采用去中心化记账以后，具体的分布式记账流程如下。\t(I) 某人发起一笔交易以后，他向全网广播。\t(2) 每个记账节点，持续监听、传播全网的交易。 收到一笔新交易，验证准确性以后，将其\t放入交易池，并继续向其他节点传播。\t(3) 因为网络传播，同一时间 、 不同记账节点的交易池不一定相同。\t(4) 每隔 10 分钟，从所有记账节点当中，按照某种方式抽取一个节点，将其交易池作为下一\t个区块，并向全网广播。\t(5) 其他节点根据最新的区块中的交易，删除自己交易池中已经记录的交易，继续记账， 等\t待下一次被选中。\t在上面的这个分布式记账的步骤中，还有一个很重要的问题就是如何分配记账权 在比特币\t60\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 72\n",
      "<class 'str'>\n",
      "l 第 2 章 大数据与其他新兴技术的关系仁二二二二\t区块链中，采用的是工作量证明 (Proof of Work, PoW ) 机制来分配记账权。记账节点通过计算\t数学题（见图 2-32) 来争夺记账权。\t找到朵随机数，使得以卜小等式成立\tSHA-256哈希函数（随机数， 父区块哈希伯，交易池中的交易） ＜朵一指定位\t图 2-32 PoW机制的数学原理\t计算上面这个数学公式，除了从零开始遍历随机数碰运气以外，没有其他办法。 解题的过程\t又叫＂挖矿＂，记账节点被称为＂矿丁产。 谁先解对，谁就获得记账权。 某记账节点率先找到解，\t就向全网公布，其他节点验证无误之后，将该区块列入区块链，重新开始下一轮计算，这种机制\t被称为 “PoW”。\t总而言之，比特币的全貌就是，采用区块链（数据结构＋哈希函数），保证账本不能被篡改；\t采用数字签名技术，保证只有自己才能够使用自己的账户；采用 P2P 网络和 PoW 共识机制，保\t证去中心化的运作方式。\t2.5.3 区块链的定义\t前面以比特币为例对区块链原理做了基本介绍，现在就可以来讲解区块链的定义。 区块链是\t利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码\t学的方式保证数据传输和访问安全的一种全新的分布式基础架构与计算范式。\t区块链的二要素是交易、区块和链， 具体如下。\t(1) 交易 ： 是一次操作，它会导致账本状态的一次改变，如添加一条记录。\t(2) 区块：一个区块记录了一段时间内发生的交易和状态结果，是对当前账本状态的一次\t共识。\t(3) 链：由一个个区块按照发生顺序串联而成，是整个状态变化的日志记录。\t可以看出，区块链的本质就是分布式账本，是一种数据库。 区块链用哈希算法实现信息不可\t篡改，用公钥、私钥来标识身份，以去屯心化和去中介化的方式，来集体维护一个可靠数据库。\t2.5.4 区块链的应用\t从科技层面来看，区块链涉及数学、密码学、互联网和计算机编程等很多科学技术问题。 从\t应用视角来看，简单来说，区块链是一个分布式的共享账本和数据库，具有去中心化、 不可篡改、\t全程留痕、可以追溯、集体维护、公开透明等特点。 这些特点保证了区块链的“诚实”与“透明”,\t为区块链创造信任奠定了坚实的基础。 而区块链丰富的应用场景，基本上都基于区块链能够解决\t信息不对称的问题，实现多个主体之间的协作信任与一致行动。\t总体而言，区块链在各个领域的主要应用如下。\t( I ) 金融领域。 区块链在国际汇兑、 信用证明、股权登记和证券交易所等金融领域有着潜在\t61\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 72\n",
      "<class 'str'>\n",
      "l 第 2 章 大数据与其他新兴技术的关系仁二二二二\t区块链中，采用的是工作量证明 (Proof of Work, PoW ) 机制来分配记账权。记账节点通过计算\t数学题（见图 2-32) 来争夺记账权。\t找到朵随机数，使得以卜小等式成立\tSHA-256哈希函数（随机数， 父区块哈希伯，交易池中的交易） ＜朵一指定位\t图 2-32 PoW机制的数学原理\t计算上面这个数学公式，除了从零开始遍历随机数碰运气以外，没有其他办法。 解题的过程\t又叫＂挖矿＂，记账节点被称为＂矿丁产。 谁先解对，谁就获得记账权。 某记账节点率先找到解，\t就向全网公布，其他节点验证无误之后，将该区块列入区块链，重新开始下一轮计算，这种机制\t被称为 “PoW”。\t总而言之，比特币的全貌就是，采用区块链（数据结构＋哈希函数），保证账本不能被篡改；\t采用数字签名技术，保证只有自己才能够使用自己的账户；采用 P2P 网络和 PoW 共识机制，保\t证去中心化的运作方式。\t2.5.3 区块链的定义\t前面以比特币为例对区块链原理做了基本介绍，现在就可以来讲解区块链的定义。 区块链是\t利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码\t学的方式保证数据传输和访问安全的一种全新的分布式基础架构与计算范式。\t区块链的二要素是交易、区块和链， 具体如下。\t(1) 交易 ： 是一次操作，它会导致账本状态的一次改变，如添加一条记录。\t(2) 区块：一个区块记录了一段时间内发生的交易和状态结果，是对当前账本状态的一次\t共识。\t(3) 链：由一个个区块按照发生顺序串联而成，是整个状态变化的日志记录。\t可以看出，区块链的本质就是分布式账本，是一种数据库。 区块链用哈希算法实现信息不可\t篡改，用公钥、私钥来标识身份，以去屯心化和去中介化的方式，来集体维护一个可靠数据库。\t2.5.4 区块链的应用\t从科技层面来看，区块链涉及数学、密码学、互联网和计算机编程等很多科学技术问题。 从\t应用视角来看，简单来说，区块链是一个分布式的共享账本和数据库，具有去中心化、 不可篡改、\t全程留痕、可以追溯、集体维护、公开透明等特点。 这些特点保证了区块链的“诚实”与“透明”,\t为区块链创造信任奠定了坚实的基础。 而区块链丰富的应用场景，基本上都基于区块链能够解决\t信息不对称的问题，实现多个主体之间的协作信任与一致行动。\t总体而言，区块链在各个领域的主要应用如下。\t( I ) 金融领域。 区块链在国际汇兑、 信用证明、股权登记和证券交易所等金融领域有着潜在\t61\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 78\n",
      "<class 'str'>\n",
      "1 第 3章 大数据基础知识 仁二二二二\t3.1.1 传统数据安全\t数据作为一种资源，它的普遍性、共享性、增值性、可处理性和多效用性，使其对人类具有\t特别重要的意义。 数据安全的实质就是保护信息系统或信息网络中的数据资源免受各种类型的威\t胁、干扰和破坏，即保证数据的安全性。\t传统的数据安全的威胁主要包括以下 3 个方面。\t(l) 计算机病毒。 计算机病毒能影响计算机软件、硬件的正常运行，破坏数据的正确与完整，\t甚至导致系统崩溃等严重的后果，特别是一些盗取各类数据信息的木马病毒等。 目前杀毒软件普\t及较广（比如免费的 360 杀毒软件），计算机病毒造成的数据信息安全隐患得到了很大程度的缓解。\t(2) 黑客攻击。 计算机入侵、账号泄漏、资料丢失、网页被黑等也是企业信息安全管理中经\t常遇到的问题。 黑客攻击往往具有明确的目标。 当黑客要攻击一个目标时，通常首先收集被攻击\t方的有关信息，分析被攻击方可能存在的淜洞，然后建立模拟环境，进行模拟攻击，测试对方可\t能的反应，再利用工具进行扫描，最后通过已知的漏洞，实施攻击。 攻击成功后就可以读取邮件，\t搜索和盗窃文件，毁坏重要数据，破坏整个系统的信息，造成不堪设想的后果。\t(3) 数据信息存储介质的损坏。 在物理介质层次上对存储和传输的信息进行安全保护，是信\t息安全的基本保障。 物理安全隐患大致包括 3 个方面：一是自然灾害（如地震、 洪水、雷电等）、\t物理损坏（如硬盘损坏、设备使用到期、外力损坏等）和设备故障（如停电断电等）；二是电磁辐\t射、信息泄漏、痕迹泄露（如口令、密钥等保管不善）； 三是操作失误（如删除文件、格式化硬盘、\t线路拆除）、 发生意外、人员疏漏等。\t3.1.2 大数据安全与传统数据安全的不同\t传统的信息安全理论重点关注数据作为资料的保密性、 完整性和可用性（即“三世＇）等静态\t安全特性，其受到的主要威胁在于数据泄露、篡改、灭失所导致的“三性“破坏。 随着信息化和\t信息技术的进一步发展，信息社会从小数据时代进入更高级的形态 大数据时代。 在此阶段，\t通过共享、交易等流通方式，数据质扯和价值得到更大程度的实现和提升，数据动态利用逐渐走\t向常态化、 多元化，这使大数据安全表现出与传统数据安全不同的特征，具体来说有以下几个\t方面。\t(I) 大数据成为网络攻击的显著目标\t在网络空间中，数据越多，受到的关注也越高，因此，大数据是更容易被发现的大目标。一\t方面，大数据对千潜在的攻击者具有较大的吸引力，因为大数据不仅量大，而且包含了大批复杂\t和敏感的数据；另一方面，当数据在一个地方大量聚集以后，安全屏障一旦被攻破，攻击者就能\t一次性获得较大的收益。\t(2) 大数据加大隐私泄露风险\t从大数据技术角度看， Hadoop 等大数据平台对数据的聚合增加了数据泄露的风险。 Hadoop\t作为一个分布式系统架构，具有海址数据的存储能力，存储的数据量可以达到 PB 级别； 一旦数\t67\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 86\n",
      "<class 'str'>\n",
      "1 第 3 章大数据基础知识仁＝二二二\t三大要素的联动。\t本节首先介绍传统的思维方式，并指出大数据时代需要新的思维方式，然后介绍大数据思维\t方式，包括全样而非抽样、效率而非精确、相关而非因果、以数据为中心、“我为人人，人人为我”\t等，最后给出运用大数据思维的具体实例。\t3.2.1 传统的思维方式\t机械思维可以追溯到古希腊思辨的思想和逻辑推理的能力，最有代表的是欧几里得的几何学\t和托勒密的地心说。\t不论是经济学家，还是之前的托勒密、牛顿等人，他们都遵循机械思维。 如果我们把他们的\t方法论做一个简单的概括，其核心思想有如下两点：首先，需要有一个简单的元模型，这个模型\t可能是假设出来的，然后用这个元模型构建复杂的模型；其次，整个模型要和历史数据相吻合。\t这在今天的动态规划管理学上还被广泛地使用，其核心思想和托勒密的方法论是一致的。\t后来人们将牛顿的方法论概括为机械思维，其核心思想可以概括成以下 3 点。\t第一，世界变化的规律是确定的，这一点从托勒密到牛顿，都认可。\t第二，因为有确定性做保障，所以规律不仅可以被认识，而且可以用简单的公式或者语言描\t述清楚。 这一点在牛顿之前，大部分人并不认可，而是简单地把规律归结为神的作用。\t第三，这些规律应该是放之四海而皆准的，可以应用到各种未知领域指导实践，这种认识是\t在牛顿之后才有的。\t这些其实是机械思维中积极的部分。机械思维更广泛的影响是作为一种准则指导人们的行为，\t其核心思想可以概括成确定性（或者可预测性）和因果关系。 在牛顿经典力学体系中，可以把所\t有天体运动的规律用几个定律讲清楚，并且应用到任何场合都是正确的，这就是确定性。 类似地，\t当我们给物体施加一个外力时，它获得一个加速度，而加速度的大小取决于外力和物体本身的质\t量， 这是一种因果关系。 没有这些确定性和因果关系，我们就无法认识世界。\t3.2.2 大数据时代需要新的思维方式\t人类社会的进步在很大程度上得益于机械思维，但是到了信息时代，它的局限性越来越明显。\t首先，并非所有的规律都可以用简单的原理来描述；其次，像过去那样找到因果关系规律性巳经\t变得非常困难，因为简单的因果关系规律性都已经被发现了，剩下那些没有被发现的因果关系规\t律性，具有很强的隐蔽性，发现的难度很高。 另外，随着人类对世界认识得越来越清楚，人们发\t现世界本身存在着很大的不确定性，并非如过去想象的那样一切都是可以确定的。 因此，在现代\t社会里，人们开始考虑在承认不确定性的清况下如何取得科学上的突破，或者把事情做得更好，\t这也就导致一种新的方法论诞生。\t不确定性在我们生活的世界里无处不在。 我们经常可以看到这样一种怪现象，很多时候专家\t们对未来各种趋势的预测是错的，这在金融领域尤其常见。 如果读者有心统计一些经济学家们对\t未来的看法，就会发现它们基本上是对错各一半。 这并不是因为他们缺乏专业知识，而是由于不\t75\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 92\n",
      "<class 'str'>\n",
      "l 第 3章大数据基础知识 仁＝＝＝\t产率，降低交易成本，为终端的用户带去更多价值。\t在这类模式下｀尽管一些零售商的利润会受到挤压，但从商业本质上来讲，这类模式可以把\t钱更多地放回到用户的口袋里，让购物变得更理性。 这是依靠大数据催生出的一项全新产业。 这\t家为数以万计的用户省钱的公司，后来被 eBay 高价收购。\t2. 啤酒与尿布\t＂啤酒与尿布＂的故事和全球最大的零售商沃尔玛有关。 沃尔玛的工作人员在按周期统计产品\t的销售信息时． 发现了一个非常奇怪的现象： 每到周末的时候，超市甩啤酒和尿布的销批就会突\t然增大。 为了搞清楚其中的原因，他们派出主作人员进行调查。 通过观察和走访之后，他们了解\t到，在美国有孩子的家庭中，太太经常嘱咐丈夫下班后要为孩子买尿布，而丈夫们在买完尿布以\t后顺手带回了自己爱喝的啤酒（休息时喝酒是很多男人的习-惯），因此-，周末时啤酒和尿布销量一\t起增长（见图 3-2)。 弄明白原因后，沃尔玛打破常规，\t／令\t尝试将啤酒和尿布摆在一起，结果使啤酒和尿布的销 TOl 啤酒\t批双双激增，为公司带来了巨大的利润。通过这个故 TOl 尿布 ~,\tT02 啤酒\t事我们可以看出，本来尿布与啤酒是两个“风马牛不 T02 尿布\tT03 尿布 心\t相及＂的东西，但如果关联在一起，销批就增加了。\t图 3-2 在超市里啤酒与尿布常常被一起购买\t3. 零售商 Target 的基于大数据的商品营销\t美国人逛超市，除了大家熟悉的沃尔玛，还有美国第＝大零售商 Target。 一个真实的故事：\t一天， 一名美国男子闯入他家附近的 Target, 抗议说超市竞然给他 17 岁的女儿发婴儿尿布和童车\t的优惠券，这是赤裸裸的侮辱，他要起诉超市。 店铺经理立刻跑出来承认错误， 一脸惜懂的经理\t也不知道发生了什么事。一个月以后这位父亲又跑来道歉，这个时候他才知道他的女儿的确怀孕\t仁 Target 比他知道他女儿怀孕足足早了一个月，那么 Target 是怎么知道的呢（这个女孩也没有\t买过任何母婴用品啊） ？原来这就是神秘的大数据起的作用。 Target 做了什么呢） ？它从数据仓\t库中挖掘出了 25 项与怀孕高度相关的商品，制作了一个怀孕预测的指数，根据指数能够在很小的\t误差范围内预测顾客有没有怀孕。 实际上这个女孩只是买了一些没有味道的湿纸巾和一些补镁的\t药品，就被 Target锁定为怀孕者了。\t4. 吸烟有害身体健康的法律诉讼\t在过去，由于数据批有限，而且常常不是多维度的，这样的相关性很难找得到．即使偶尔找\t到了．人们也未必接受，因为这和传统的观念不一样。 20 世纪 90 年代中期，在美国和加拿大伟l\t绕香烟是否对人体有害这件事情的一系列诉讼上，如何判定吸烟有害是这些案子的关键，采用因\t果关系判定， 还是采用相关性判定， 决定了那些诉讼案的判决结果。\t在今天的人看来，吸烟对人体有害，这是板上钉钉的事实（见佟I 3-3)。 比如美国外科协会的\t一份研究报告显示，吸烟男性肺癌的发病率是不吸烟男性的 231音吸烟女性肺癌的发病率则是不\t吸烟女性的 13 倍，这从统计学上讲早已经不是偶然的随机事件了，而是存在必然的联系。 但是，\t就是这样看似如山的铁证，依然”不足够＂判定烟草公司有罪，因为它们认为吸烟和肺癌没有因\t果关系 烟草公司可以找出很多理由来辩解，比如说一些人之所以要吸烟、是因为身体里有某部\t81\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 91\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t踪足够多的人出行情况的实时信息的工具，一个城市即使部署再多的采样观察点，再频繁地报告\t各种交通事故和拥堵的情况，整体交通路况信息的实时性也不会有很大提高。\t但是，在能够定位的智能手机出现后，这种情况得到了根本的改变。 由千智能手机足够普及\t并且大部分用户共享了他们的实时位置信息（符合大数据的完备性），使得做地图服务的公司，比\t如 Google 或者百度，有可能实时地得到任何一个人口密度较大的城市的人员流动信息，并且根据\t其流动的速度和所在的位置，区分步行的人群和行进的汽车。\t由于收集信息的公司和提供地图服务的公司是一家，因此从数据采集、数据处理，到信息发\t布，中间的延时微乎其微，提供的交通路况信息要及时得多。 使用过 Google地图服务或者百度地\t图服务的人，对比智能手机和智能汽车出现前，都很明显地感到了其中的差别。 当然，更及时的\t信息可以通过分析历史数据来预测。 一些科研小组和公司的研发部门，已经开始利用一个城市交\t通状况的历史数据，结合实时数据，预测一段时间以内（比如一个小时）该城市各条道路可能出\t现的交通状况，并且帮助出行者规划最好的出行路线。\t上面的实例很好地阐释了大数据时代“我为人人，人人为我”的全新理念和思维。 每个使用\t导航软件的智能手机用户，一方面共享自己的实时位控信息给导航软件公司（比如百度地图），使\t得导航软件公司可以从大拭用户那里获得实时的交通路况大数据；另一方面，每个用户又在享受\t导航软件公司提供的基于交通大数据的实时导航服务。\t3.2.4 运用大数据思维的具体实例\t为了进一步强化对大数据思维的理解，这里给出大数据思维及其代表性实例（见表 3-1)。\t表·3-1 大数挹思维及其代表性实例\t思维方式 具体实例\t全样而非抽样 商品比价网站\t效率而非精确 Google 翻译\t啤酒与尿布\t零售商Target 的基千大数据的商品营销\t相关而非因果\t吸烟有害身体健康的法律诉讼\t基千大数据的药品研发\t基于大数据的 Google广告\t以数据为中心 搜索引擎“点击模型”\t大数据的简单贷法比小数据的复杂算法更有效\t“我为人人，人人为我” 迪士尼乐园 MagicBand 手环\t1. 商品比价网站\t美国有一家创新网站，它可以预测产品的价格趋势，帮助用户做购买决策，告诉用户什么时\t候买什么产品，什么时候买最便宜。 这家公司背后的驱动力就是大数据。 他们在全球各大网站上\t搜集数以十亿计的数据，然后帮助数以万计的用户省钱，为他们的采购找到最好的时间｀提高生\t80\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 275\n",
      "<class 'str'>\n",
      "参考文献\t[I] 林子雨大数据导论（通识课版） [M]. 北京：高等教育出版补， 2020\t[2] 林子雨大数据技术原理与应用［M]. 第 2 版北京：人民邮电出版社， 2017.\t[3] 林子雨 大数据基础编程、实验和案例教程[M]．北京：清华大学出版社， 2017.\t[4] 林子雨，赖永炫，陶继平． Spark 编程基础 (Scala 版 ） ［M]. 北京：人民邮电出版社， 2018.\t[5] 林子雨，郑海山，赖永炫 Spark 编程基础 (Python 版 )[M]．北京：人民邮电出版社， 2020.\t[6] 林子雨大如居实训案例一电影推荐系统 ( Scala版）［M]. 北京：人民邮电出版社， 2019.\t[7] 林子雨大数据实训案例——电信用户行为分析 (Scala 版） ［M]. 北京：人民邮电出版\t社， 2019.\t[8] 维克托·迈尔－舍恩伯格，肯尼思·库克耶．大数据时代： 生活、丁竹作与思维的大变革[M]\t盛杨燕等译杭州：浙江人民出版社， 2013.\t[9] 张敏大数据的悖论［J]. 中关村， 2018(5):91\t[10] 陈高华，蔡其胜大数据环境下精准诈骗治理难题的伦理反思[J]. 自然辩证法通讯， 2018,\t40(11):26-32.\t[11] 袁雪．大数据技术的伦理 ”七宗罪＂ ［J]. 科技传播， 2016,8(7):89-90\t[12] 郭胜大数据技术的伦理问题反思［J]. 科技传播， 2018,10(19):4-7\t[13] 宋吉鑫大数据技术的伦理问题及治理研究[J]. 沈阳工程学院学报（社会科学版）， 2018,\t14(4):452-455.\t[14] 朱沁卉．大数据技术的伦理问题探究［J]. 科技风， 2018(24):78+80\t[ 15] 陈艳， 李君亮，栾忠恒，夏澜英．大数据技术伦理：问题、根源及对策[J]．金华职业技\t术学院学报， 2016,16(6):90-92.\t[16] 田维琳大数据伦理失范问题的成因与防范研究［J]. 思想教育研究， 2018(8):107-111.\t[17] 宋吉鑫大数据技术的伦理问题及治理研究[J] 沈阳T一程学院学报（社会科学版）， 2018,\t14(04):452-455.\t(18] 李航大数据时代：网络隐私伦理问题探究［J]. 现代商业， 2018(29):165-166\t[19] 陈仕伟大数据时代数字鸿沟的伦理治理[J] 创新， 2018,12(3): 15-22.\t[20] 李俏．大数据时代下的隐私伦理建构研究[J]．儿江学院学报（社会科学版）．\t2018,37(04): 106-109.\t[21] 王永峰对大数据道德悖论的思考[J]． 人力资源管理， 2016(1):201-202.\t[22] 王强芬儒家伦理对大数据隐私伦理构建的现代价值［J]. 医学与哲学， 2019,40(1):30-34\t[23] 杨欣试论大数据垄断的法律规制［J]. 法制博览， 2018(3):145+144\t[24] 鲁浪浪大数据交易的规则体系构建研究[J] 中小企业管理与科技 2017(12): 180-182.\t264\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 100\n",
      "<class 'str'>\n",
      "l 第 3章大数据基础知识 仁二二＝二\t的一些小型坑洼，长期得不到及时处理。\t很显然，在这个例子中，具备智能手机使用能力的群体相对于不会使用智能手机的群体而言，\t具有明显的比较优势，可以及时把自己群体的诉求表达出来，获得关注和解决，而后者的诉求则\t无法及时得到响应。\t4. “信息茧房＇问题\t我们日常生活中的很多决策，都需要我们综合多方面的信息去做判断。 如果对世界的认识存\t在偏差，做出的决策肯定会有错误。 也就是说，如果我们只是看某一方面的信息，对另一方面的\t信息视而不见，或者永远怀着怀疑、 批判的眼光去看与自己观点不同的信息，那么，我们就有可\t能做出偏颇的决策。\t现在的互联网，基于大数据和人工智能的推荐应用越来越多，越来越深入。 每一个应用软件\t的背后，几乎都有一个庞大的团队，时时刻刻在研究我们的兴趣爱好，然后推荐我们喜欢的信息\t来迎合我们的需求。 久而久之，我们一直被“喂食着“经过智能化筛选推荐的信息，就会导致我\t们被封闭在一个“信息茧房”里面，看不见外面丰富多彩的世界。\t我们日常生活中使用的“今日头条”等手机 App 就是典型的代表。 今日头条是一款基于数据\t挖掘的推荐引擎产品，它为用户推荐有价值的、个性化的信息，提供连接人与信息的新型服务。\t今日头条的本质是： 依靠数据挖掘，提供个性化、有价值的信息。 用户在今日头条产生阅读记录\t以后，今日头条就会根据用户的喜好，不断推荐用户喜欢的内容供用户观看，对用户不喜欢的内\t容，进行高效的屏蔽，用户永远看不到他不感兴趣的内容。 于是，在今日头条中，我们的视野，\t就永远被局限在一个非常狭小的范围内，我们关注的那一方面内容，就成了一个“信息茧房”，把\t我们严严实实地包裹在里面，对于外面的一切，我们一无所知。 时间一长，今日头条不仅在取悦\t用户，同时也在＂驯化”用户。 用户在起初是主人，到了后来，就变成了＂奴隶\"。\t实际上，在 2016 年的美国总统大选中，很多美国人就尝到了“信息茧房”的苦果。 当时，在\t选举结果揭晓之前，美国东部的教授、学生、金融界人士和西部的演艺界、互联网界、科技界人\t士，基本上都认为希拉里稳赢，在他们看来，特朗普没有任何胜算。 希拉里的拥歪们早早就准备\t好了庆祝希拉里获胜的庆典和物品，就等着投票结果出来。 教授和学生们在教室里集体观看电视\t直播，等着最后的狂欢。 但是，选举结果却完全出乎这些东西部的精英们的意料，因为特朗普最\t终胜出，当选了总统。 他们无论如何也无法摘懂，根据他们平时所接触到的信息来判断，几乎身\t边的所有人都喜欢希拉里，为什么赢的却是特朗普呢？这个问题的答案就在于，这些精英们被关\t在了一个“信息茧房”里． 因为他们喜欢希拉里，所以， Facebook等网络应用都会为他们推荐各\t种各样支持希拉里的文章，自动屏蔽那些支持特朗普的文章，他们全都坚定地认为，大部分人都\t支持希拉里，只有极少数人会支持特朗普。 可是，事实的真相完全不是这样。 根据美国总统大选\t期间的统计数字，就在 Facebook 上，特朗普的支持者数掀远远超过这些东西部精英们的想象，只\t不过这些精英们生活在一个“信息茧房”中，看不见特朗普支持者的存在。 比如， 有一篇名为《我\t为什么要投票给特朗普》的文章， 在 Facebook上被分享超过 150 万次，可是很多东西部的精英们\t居然没有听说过这篇文章。 所以，生活在大数据时代，我们一定要高度警惕自己落入“信息茧房”\t89\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 78\n",
      "<class 'str'>\n",
      "1 第 3章 大数据基础知识 仁二二二二\t3.1.1 传统数据安全\t数据作为一种资源，它的普遍性、共享性、增值性、可处理性和多效用性，使其对人类具有\t特别重要的意义。 数据安全的实质就是保护信息系统或信息网络中的数据资源免受各种类型的威\t胁、干扰和破坏，即保证数据的安全性。\t传统的数据安全的威胁主要包括以下 3 个方面。\t(l) 计算机病毒。 计算机病毒能影响计算机软件、硬件的正常运行，破坏数据的正确与完整，\t甚至导致系统崩溃等严重的后果，特别是一些盗取各类数据信息的木马病毒等。 目前杀毒软件普\t及较广（比如免费的 360 杀毒软件），计算机病毒造成的数据信息安全隐患得到了很大程度的缓解。\t(2) 黑客攻击。 计算机入侵、账号泄漏、资料丢失、网页被黑等也是企业信息安全管理中经\t常遇到的问题。 黑客攻击往往具有明确的目标。 当黑客要攻击一个目标时，通常首先收集被攻击\t方的有关信息，分析被攻击方可能存在的淜洞，然后建立模拟环境，进行模拟攻击，测试对方可\t能的反应，再利用工具进行扫描，最后通过已知的漏洞，实施攻击。 攻击成功后就可以读取邮件，\t搜索和盗窃文件，毁坏重要数据，破坏整个系统的信息，造成不堪设想的后果。\t(3) 数据信息存储介质的损坏。 在物理介质层次上对存储和传输的信息进行安全保护，是信\t息安全的基本保障。 物理安全隐患大致包括 3 个方面：一是自然灾害（如地震、 洪水、雷电等）、\t物理损坏（如硬盘损坏、设备使用到期、外力损坏等）和设备故障（如停电断电等）；二是电磁辐\t射、信息泄漏、痕迹泄露（如口令、密钥等保管不善）； 三是操作失误（如删除文件、格式化硬盘、\t线路拆除）、 发生意外、人员疏漏等。\t3.1.2 大数据安全与传统数据安全的不同\t传统的信息安全理论重点关注数据作为资料的保密性、 完整性和可用性（即“三世＇）等静态\t安全特性，其受到的主要威胁在于数据泄露、篡改、灭失所导致的“三性“破坏。 随着信息化和\t信息技术的进一步发展，信息社会从小数据时代进入更高级的形态 大数据时代。 在此阶段，\t通过共享、交易等流通方式，数据质扯和价值得到更大程度的实现和提升，数据动态利用逐渐走\t向常态化、 多元化，这使大数据安全表现出与传统数据安全不同的特征，具体来说有以下几个\t方面。\t(I) 大数据成为网络攻击的显著目标\t在网络空间中，数据越多，受到的关注也越高，因此，大数据是更容易被发现的大目标。一\t方面，大数据对千潜在的攻击者具有较大的吸引力，因为大数据不仅量大，而且包含了大批复杂\t和敏感的数据；另一方面，当数据在一个地方大量聚集以后，安全屏障一旦被攻破，攻击者就能\t一次性获得较大的收益。\t(2) 大数据加大隐私泄露风险\t从大数据技术角度看， Hadoop 等大数据平台对数据的聚合增加了数据泄露的风险。 Hadoop\t作为一个分布式系统架构，具有海址数据的存储能力，存储的数据量可以达到 PB 级别； 一旦数\t67\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 275\n",
      "<class 'str'>\n",
      "参考文献\t[I] 林子雨大数据导论（通识课版） [M]. 北京：高等教育出版补， 2020\t[2] 林子雨大数据技术原理与应用［M]. 第 2 版北京：人民邮电出版社， 2017.\t[3] 林子雨 大数据基础编程、实验和案例教程[M]．北京：清华大学出版社， 2017.\t[4] 林子雨，赖永炫，陶继平． Spark 编程基础 (Scala 版 ） ［M]. 北京：人民邮电出版社， 2018.\t[5] 林子雨，郑海山，赖永炫 Spark 编程基础 (Python 版 )[M]．北京：人民邮电出版社， 2020.\t[6] 林子雨大如居实训案例一电影推荐系统 ( Scala版）［M]. 北京：人民邮电出版社， 2019.\t[7] 林子雨大数据实训案例——电信用户行为分析 (Scala 版） ［M]. 北京：人民邮电出版\t社， 2019.\t[8] 维克托·迈尔－舍恩伯格，肯尼思·库克耶．大数据时代： 生活、丁竹作与思维的大变革[M]\t盛杨燕等译杭州：浙江人民出版社， 2013.\t[9] 张敏大数据的悖论［J]. 中关村， 2018(5):91\t[10] 陈高华，蔡其胜大数据环境下精准诈骗治理难题的伦理反思[J]. 自然辩证法通讯， 2018,\t40(11):26-32.\t[11] 袁雪．大数据技术的伦理 ”七宗罪＂ ［J]. 科技传播， 2016,8(7):89-90\t[12] 郭胜大数据技术的伦理问题反思［J]. 科技传播， 2018,10(19):4-7\t[13] 宋吉鑫大数据技术的伦理问题及治理研究[J]. 沈阳工程学院学报（社会科学版）， 2018,\t14(4):452-455.\t[14] 朱沁卉．大数据技术的伦理问题探究［J]. 科技风， 2018(24):78+80\t[ 15] 陈艳， 李君亮，栾忠恒，夏澜英．大数据技术伦理：问题、根源及对策[J]．金华职业技\t术学院学报， 2016,16(6):90-92.\t[16] 田维琳大数据伦理失范问题的成因与防范研究［J]. 思想教育研究， 2018(8):107-111.\t[17] 宋吉鑫大数据技术的伦理问题及治理研究[J] 沈阳T一程学院学报（社会科学版）， 2018,\t14(04):452-455.\t(18] 李航大数据时代：网络隐私伦理问题探究［J]. 现代商业， 2018(29):165-166\t[19] 陈仕伟大数据时代数字鸿沟的伦理治理[J] 创新， 2018,12(3): 15-22.\t[20] 李俏．大数据时代下的隐私伦理建构研究[J]．儿江学院学报（社会科学版）．\t2018,37(04): 106-109.\t[21] 王永峰对大数据道德悖论的思考[J]． 人力资源管理， 2016(1):201-202.\t[22] 王强芬儒家伦理对大数据隐私伦理构建的现代价值［J]. 医学与哲学， 2019,40(1):30-34\t[23] 杨欣试论大数据垄断的法律规制［J]. 法制博览， 2018(3):145+144\t[24] 鲁浪浪大数据交易的规则体系构建研究[J] 中小企业管理与科技 2017(12): 180-182.\t264\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 103\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t锁、电饭倭等。 这些物联网化的智能家居产品，为我们的生活增添了很多乐趣，提供了各种便利，\t营造出更加舒适温馨的生活氛围。 但是，部分智能家居产品存在安全问题也是不争的事实，使用\t户的数据安全面临极大的风险，容易造成用户隐私的泄露。 比如，部分网络摄像头产品被黑客攻\t破，黑客可以远程随意查看相关用户的网络摄像头的视频内容。\t3. 数字鸿沟问题\t“数字鸿沟”(DigitalDivide)是 1995 年美国国家远程通信和信息管理局(NationalTelecommunications\tand Information Administration) 发布的《被互联网遗忘的角落 一项关于美国城乡信息穷人的\t调查报告》最早提出的。 数字鸿沟总是指向信息时代的不公平，尤其在信息基础设施、信息工具\t以及信息的获取与使用等领域，或者可以认为是信息时代的“马太效应”，即先进技术的成果不能\t为人公正分享，于是造成“富者越富、穷者越穷＂的清况。\t虽然大数据时代的到来给我们的生产、生活、学习与工作带来了颠覆性的变革，但是数字鸿\t沟并没有因为大数据技术的诞生而趋向弥合。 一方面，大数据技术的基础设施并没有在全国范围\t内全面普及，更没有在世界范围内全面普及，往往是城市优于农村、经济发达地区优千经济欠发\t达地区、富国优于穷国。 另一方面，即使在大数据技术设施比较完备的地方，也并不是所有的个\t体都能充分地掌握和运用大数据技术，个体之间也存在着严重的差异。\t“数字鸿沟“正在不断地扩大，大数据技术让不同国家、不同地区、不同阶层的人们深深地\t感受到了不平等。 根据 2017 年年末的互联网普及率调查报告显示，加拿大的互联网普及率为\t94.70%，而印度的互联网普及率为 28.30%，埃塞俄比亚的互联网普及率为 4.40%，尼日尔的互联\t网普及率为 2.40%，全球平均水平仅为 47%。 大数据技术深刻依赖于底层的互联网技术，因此，\t互联网普及率的不均衡所带来的直接结果就是数据资源接受的不均衡，互联网普及率高的地方，\t能够充分利用大数据资源来改善生产和生活，而普及率低的地方无法做到这一点。 在某种程度上，\t这也是一个国家能够富强的关键，特别是在大数据时代的今天。 在我国，东中西部地区、城乡之\t间等都可以明显感受到数字鸿沟的存在。\t数字鸿沟是一个涉及公平公正的问题。 在大数据时代，每一个人原则上都可以由一连串的数\t字符号来表示，从某种程度上来说，数字化的存在就是人的存在。 因此，数字信息对于人来说就\t成了一个非常重要的存在。 每一个人都希望能够享受大数据技术所带来的福利，而不仅是某些国\t家、公司或者个人垄断大数据技术的相关福利。 如果只有少部分人能够较好地占有并较完整地利\t用大数据信息，而另外一部分人却难以接受和利用大数据资源，会造成数据占有的不公平。 而数\t据占有的程度不同，又会产生信息红利分配不公平等问题，加剧群体差异，导致社会矛盾加剧。\t因此，必须要思考解决“数字鸿沟”这一伦理问题，实现均衡而又充分的发展。\t4. 数据独裁问题\t所谓的“数据独裁”是指在大数据时代，由于数据最的爆炸式增长，导致做出判断和选择的\t难度陡增，迫使人们必须完全依赖数据的预测和结论才能做出最终的决策。 从某个角度来讲，就\t是让数据统治人类，使人类彻底走向唯数据主义。 在大数据时代，在大数据技术的助力之下，人\t工智能获得了长足的发展， 机器学习和数据挖掘的分析能力越来越强大，预测越来越精准。 比如，\t92\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 172\n",
      "<class 'str'>\n",
      "1 第 5 章数据采集与预处理仁二二二二\t虚拟机 (Java Virtual Machine, NM)。 Agent是一个完整的数据采集工具，包含三个核心组件，\t分别是数据源 (Source)、数据通道 (Channel) 和数据槽 (Sink) （见图 5-1)。 通过这些组件，”事\t件”(Event) 可以从一个地方流向另一个地方。 每个组件的具体功能如下。\t(1) 数据源是数据的收集端，负责将数据捕获后进行特殊的格式化处理，将数据封装到事件\t里，然后将事件推入数据通道。\t(2) 数据通道是连接数据源和数据槽的组件，可以将它看作一个数据的缓冲区（数据队列）。\t它可以将事件暂存到内存，也可以持久化保存到本地磁盘上，直到数据槽处理完该事件。\t(3) 数据槽取出数据通道中的数据，存储到文件系统和数据库，或者提交到远程服务器。\t三\tChannel\tAgent\t图 5-1 Flume 的核心组件\t2. 分布式消息订阅分发\t分布式消息订阅分发也是一种常见的数据采集方式，其中， Kafka 就是一种具有代表性的产\t品。 Kafka 是由 Linkedln 公司开发的一种高吞吐量的分布式发布／订阅消息系统。 用户通过 Kafka\t系统可以发布大量的消息，同时也能实时订阅消费消息。 Kafka 设计的初衷是构建一个可以处理\t海屈日志、 用户行为和网站运营统计等的数据处理框架。 为了满足上述应用需求，数据处理框架\t就需要同时提供实时在线处理的低延迟和批量离线处理的高吞吐蜇等功能。 现有的一些数据处理\t框架，通常设计了完备的机制来保证消息传输的可靠性，但是由此会带来较大的系统负担，在批\t址处理海最数据时无法满足高吞吐量的要求。另外有一些数据处理框架则被设计成实时消息处理\t系统，虽然可以带来很高的实时处理性能，但是在批量离线场合时无法提供足够的持久性，即可\t能发生消息丢失。 同时，在大数据时代涌现的新的日志收集处理系统（如 Flume、 Scribe 等）往\t往更擅长批屋离线处理，而不能较好地支持实时在线处理。 相对而言， Kafka 可以同时满足在线\t实时处理和批量离线处理的要求。\tKafka 的架构包括以下组件（见图 5-2)。\t(I) 话题 (Topic)：特定类型的消息流。\t(2) 生产者 (Producer)：能够发布消息到话题的任何对象。\t(3) 服务代理 (Broker)：保存已发布的消息的服务器，被称为代理或 Kafka集群。\t(4) 消费者 (Consumer)：可以订阅一个或多个话题，并从服务代理拉数据，从而“消费”\t这些已发布的消息。\t从图 5-2 中可以看出，生产者将数据发送到服务代理，服务代理有多个话题，消费者从服务\t代理获取数据。\t161\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 175\n",
      "<class 'str'>\n",
      "二 大数据导论 I\t3. 网络爬虫的类型\t网络爬虫可以分为通用网络爬虫、聚焦网络爬虫、增掀式网络爬虫、深层网络爬虫 4 种类型＾\t(l) 通用网络爬虫。 通用网络爬虫又称“全网爬虫”(Scalable Web Crawler)，抓取对象从一\t些种子 URL 扩充到整个 Web，该类爬虫主要为门户站点搜索引擎和大型 Web 服务提供商采集数\t据。 通用网络爬虫的结构大致可以包括页面抓取模块、页面分析模块、链接过滤模块、页面数据\t库、 URL 队列和初始 URL 集合。 为提高工作效率，通用网络爬虫会采取一定的抓取策略。 常用\t的抓取策略有：深度优先策略和广度优先策略等。\t(2)聚焦网络爬虫。聚焦网络爬虫(Focused Crawler)，又称“主题网络爬虫”(Topical Crawler),\t是指选择性地抓取那些与预先定义好的主题相关的页面的网络爬虫。 和通用网络爬虫相比，聚焦\t网络爬虫只需要抓取与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数鼠\t少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求。 聚焦网络爬虫的T什作流程\t较为复杂，需要根据一定的网页分析算法过滤与主题无关的 URL, 保留有用的 URL 并将其放入\t等待抓取的 URL 队列。 然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页，并重\t复上述过程，直到达到系统的特定条件时停止。 另外，所有被爬虫抓取的网页将会被系统存储，\t进行一定的分析、过滤，并建立索引，以便用于之后的查询和检索。 对于聚焦网络爬虫来说，这\t一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。 聚焦网络爬虫常用的策略包\t括： 基千内容评价的抓取策略、基千链接结构评价的抓取策略、基于增强学习的抓取策略和基于\t语境图的抓取策略等。\t(3) 增撮式网络爬虫。 增釐式网络爬虫 (Incremental Web Crawler) 是指对已下载页面采取增\t釐式更新和只抓取新产生的或者巳经发生变化页面的爬虫，它能够在一定程度上保证所抓取的页\t面是尽可能新的页面。 与周期性抓取和刷新页面的网络爬虫相比，增量式网络爬虫只会在需要的\t时候抓取新产生或发生更新的页面，并不重新下载没有发生变化的页面，可有效减少数据下载队，\t及时更新巳抓取的页面，减小时间和空间上的耗费，但是增加了抓取算法的复杂度和实现难度。\t增量式网络爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面\t的质釐。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容。\t为了实现第二个目标，增拭式爬虫需要对网页的重要性排序，常用的策略包括广度优先策略和\tPageRank 优先策略等。\t(4) 深层网络爬虫。 深层网络爬虫将 Web 页面按存在方式分为表层网页 (Surface Web) 和深\t层网页 (Deep Web, 也称 InvisibleWeb Page 或 Hidden Web)。表层网页是指传统搜索引擎可以索\t引的、超链接可以到达的静态网页。 深层网页是那些大部分内容不能通过静态链接获取的、隐藏\t在搜索表单后的、只有用户提交一些关键词才能获得的 Web 页面。 深层网络爬虫体系结构包含 6\t个基本功能模块（抓取控制器、解析器、表单分析器、表单处理器、响应分析器、 LVS 控制器）\t和两个爬虫内部数据结构 (URL列表、 LVS 表）。\t4. Scrapy 爬虫\tScrapy 是一套基于 Twisted 的异步处理框架，是纯 Python 实现的爬虫框架，用户只需要定制\t164\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 177\n",
      "<class 'str'>\n",
      "三 大数据导论 l\t＠下载器中间件(DownloaderMiddlewares)。下载器中间件是位于引擎和下载器之间的组件，\t主要用于处理引擎与下载器之间的请求和响应。 类似于自定义扩展下载功能的组件。\t＠爬虫中间件 ( SpiderMiddlewares)。 爬虫中间件是介于引擎和爬虫之间的组件，主要工作\t是处理爬虫的响应输入和请求输出。\t@）调度器中间件（ SchedulerMiddlewares)。调度器中间件是介于引擎和调度器之间的中间件，\t用于处理从引擎发送到调度器的请求和响应，可以自定义扩展和操作搜索引擎与爬虫中间“通信”\t的功能组件（如进入爬虫的请求和从爬虫出去的请求）。\t(2) Scrapy 工作流\tScrapy 工作流也叫作“运行流程”或“数据处理流程”，整个数据处理流程由 Scrapy 引擎进\t行控制，其主要的运行步骤如下。\tCD\tScrapy 引擎从调度器中取出一个 URL用于接下来的抓取。\t(2) Scrapy 引擎把 URL封装成一个请求并传给下载器。\t＠下载器把资源下载下来，并封装成应答包。\t＠ 爬虫解析应答包。\t＠如果解析出的是项目， 则交给项目管道进行进一步的处理。\t＠如果解析出的是 URL，则把 URL 交给调度器等待抓取。\t5. 反爬机制\t为什么会有反爬机制？原因主要有两点：第一，在大数据时代，数据是十分宝贵的财富，很\t多企业不愿意让自己的数据被别人免费获取，因此，它们为自己的网站设计反爬机制，防止网页\t上的数据被抓取；第二，简单低级的网络爬虫，数据采集速度快，伪装度低，如果没有反爬机制，\t它们可以很快地抓取大批数据，甚至因为请求过多，造成网站服务器不能正常工作，影响企业的\t业务开展。\t反爬机制是一把“双刃剑”，一方面可以保护企业网站的数据，但是，另一方面，如果反爬机\t制过于严格，可能会误伤到真正的用户请求，也就是真正的用户请求被错误当成网络爬虫而被拒\t绝。 如果既要和“网络爬虫“死磕，又要保证很低的误伤率，那么会增加网站搭建的成本。\t通常而言，伪装度高的网络爬虫速度慢，对服务器造成的负担也相对较小。 所以，网站反爬\t的重点是针对那种简单粗暴的数据采集行为。有时反爬机制会允许伪装度高的网络爬虫获得数据，\t毕竟伪装度很高的数据采集行为与真实用户请求没有太大差别。\t5.2\t数据清洗\t数据清洗对于获得高质盘分析结果而言，其重要性不言而喻，正所谓“垃圾数据进，垃圾数\t据出“，没有高质拔的输入数据，那么输出的分析结果价值会大打折扣，甚至没有任何价值。 数据\t清洗是指将大补原始数据中的＂脏“数据“洗掉＂，它是发现并纠正数据文件中可识别的错误的最\t166\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 180\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理 仁．~\t(4) 规范化处理：将属性值按比例缩放，使之落入一个特定的区间，比如 0.0......,1.0。 常用的\t数据规范化方法包括 Min-Max 规范化、 Z-Score规范化和小数定标规范化等。\t(5) 属性构造处理：根据已有属性集构造新的属性，后续数据处理直接使用新增的属性。 例\t如，根据已知的质批和体积屈性，计算出新的属性——密度。\t5.3.2 平滑处理\t噪声是指被测变批的一个随机错误和变化。 平滑处理旨在帮助去掉数据中的噪声。 常用的方\t法包括分箱、回归和聚类等。\t1．分箱\t分箱 ( Bin) 利用被平滑数据点的周围点（近邻点），对一组排序数据进行平滑处理，排序后\t的数据被分配到若干箱子中。\t典型的分箱方法一般有两种： 一种是等高方法，即每个箱子中元素的个数相等；另一种是等\t宽方法，即每个箱子的取值间距（左右边界之差）相同，如图 5-6 所示。\t箱子中元素的个数 箱子中元素的个数\t(a)等高方法 属性值 (b)等宽方法 属性值\t图 5-6 两种典型分箱方法\t这里给出一个实例介绍分箱。 假设有一个数据集X={4,8,15,21,21,24,25,28,34} ，这里采用基于\t平均值的等高方法对其进行平滑处理，则分箱处理的步骤如下。\t(1) 把原始数据集 X放入以下 3 个箱子。\t箱子 1: 4,8,15。\t箱子 2: 21,21,24。\t箱子 3: 25,28,34。\t(2) 分别计算每个箱子的平均值。\t箱子 1 的平均值： 9。\t箱子 2 的平均值： 22。\t箱子 3 的平均值： 29。\t(3) 用每个箱子的平均值替换该箱子内的所有元素。\t箱子 1 : 9,9,9。\t箱子 2: 22,22,22。\t箱子 3: 29,29,29。\t(4) 合并各个箱子中的元素得到新的数据集{9,9,9,22,22,22,29,29必9}。\t169\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 139\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t(l) 数据源层。 该层包含了平台的各种数据来源，包括个人用户、网络和各种机构（医院、\t独立体检机构、社区卫生服务机构、区域医疗信息平台、第三方检测机构、医保社保、新农合）。\t(2) 技术支撑层。 该层包括大数据集成、基千大数据的健康评估技术、基于大数据的个性化\t诊疗技术、数据标准、安全隐私等模块，可以实现高效的个人健康信息整合、准确的健康风险分\t析评估、直观的评测报告可视化和个性化的健康计划制订等功能。\t(3) 业务层。 平台对外提供多种服务，主要包括面向普遍人群的通用型健康服务，面向特定\t人群的主题式健康服务，面向健康服务机构的信息服务，面向决策、科研等机构的循证医学数据\t服务以及开放应用平台服务等。\t(4)交互层。 平台提供对外服务的渠道，包括门户网站、呼叫中心、移动终端和平台接入 API\t(5) 用户层。 平台的服务对象，包括个人用户、专业健康服务机构、医疗卫生机构、决策机\t构、科研机构、健康服务相关机构（保险公司、医疗器械厂商、药厂等）、疾控中心等。\t4. 平台关键技术\t基于大数据应用的综合健康服务平台通常包含以下几个方面的关键技术：医疗健康大数据集\t成、存储和处理技术；基于大数据的健康评估技术；基于大数据的个性化诊疗技术。\t(l) 医疗健康大数据集成、存储和处理技术\t综合健康服务平台数据来源广泛，包括医院、独立体检机构、社区卫生服务机构、区域医疗\t信息平台、第三方检测机构、新农合、医保社保、个人用户和网络等；平台数据内容多样，包括\t病史、体格检查、理化检查、居民基本健康档案、各类个人信息和网页等，涉及结构化数据、半\t结构化和非结构化数据；平台数据撮巨大，通常要包含 1000 万以上个人用户的各种医疗健康数据。\t广泛的数据来源、多样的数据类型、海掀的数据，构成了平台的数据基础 医疗健康大数据，\t这给数据集成和管理带来了很大的技术挑战\t(2) 基于大数据的健康评估技术\t健康管理的目的是“治未病”。 通过健康信息评估，可以对收集到的个体和群体的健康状态和\t疾病信息进行系统、综合、连续的科学分析与评价，目的是为个性化诊疗、维护、促进与改善健\t康、管理与控制健康风险提供科学依据。\t健康评估是健康管理的核心技术。 基于大数据的健康评估技术，以现代健康概念和新医学校\t式，以及中医“治未病”为指导，通过采用现代医学、现代管理学、统计分析和数据挖掘的理论\t技术，在国际上现有的健康评估和健康风险评估模型的基础上，通过综合健康服务平台的“全样\t本”医疗健康大数据分析，综合考虑人的生理、心理、行为方式、生活习惯等各方而指标，建立\t适合中国不同人群的健康状态评估模型和健康风险评估模型，对国人的个体或群体整体健康状况、\t影响健康的危险因素、疾病风险进行全面检测、评估和有效干预。 其目的是以最小的投入获取最\t大的健康效益，节省医疗资源，提升全民的健康水平和生活质量。\t(3) 基于大数据的个性化诊疗技术\t已有的诊疗手段在很大程度上均来自医学专家知识，具有非实时性、一般性和普遍性的特点\t从本质上来说，仍属于“以医疗为中心”的模式和范畴。 使用大数据技术推动诊疗模式向“以人\t128\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 170\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理仁二二二二\t发展起来的，一些经过多年发展的数据采集架构、技术和工具被继承下来，同时，由于大数据本\t身具有数据量大、数据类型丰富、处理速度快等特性，使得大数据采集又表现出不同于传统数据\t采集的一些特点（见表 5-1)。\t表5-1 传统数挹采集与大数括采渠的区别\t传统数据采集 大数据采集\t数据源 来源单一｀数据扯相对较少 来源广泛，数据扯巨大\t数据类型丰富，包括结构化、半结构化和非结构化\t数据类型 结构单一\t数据\t数据存储 关系数据库和并行数据仓库 分布式数据库，分布式文件系统\t5.1.2 数据采集的三大要点\t数据采集的三大要点如下。\t(1) 全面性。 全面性是指数据量足够具有分析价值、数据面足够支撑分析需求。 比如对于“查\t看商品详清”这一行为，需要采集用户触发时的环境信息、会话以及用户 ID, 最后需要统计这一\t行为在某一时段触发的人数、次数、人均次数、活跃比等。\t(2) 多维性。数据更重要的是能满足分析需求。 数据采集必须能够灵活、快速自定义数据的\t多种属性和不同类型，从而满足不同的分析目标要求。 比如“查看商品详情”这一行为，通过“埋\t点”，我们才能知道用户查看的商品是什么、商品价格、商品类型、商品 ID 等多个属性，从而知\t道用户看过哪些商品、什么类型的商品被查看得多、某一个商品被查看了多少次，而不仅是知道\t用户进入了商品详情页。\t(3) 高效性。 高效性包含技术执行的高效性、团队内部成员协同的高效性，以及数据分析需\t求和目标实现的高效性。 也就是说，采集数据一定要明确采集目的，带着问题搜集信息，使信息\t采集更高效、更有针对性。 此外，采集数据还要考虑数据的及时性。\t5.1.3 数据采集的数据源\t数据采集的主要数据源包括传感器数据、互联网数据、日志文件、企业业务系统数据等。\t1. 传感器数据\t传感器是一种检测装置，能感受到被测量环境的信号，并能将感受到的信号按一定规律变换\t成电信号或其他所需形式的信号输出，以满足信息的传输、处理、存储、显示、记录和控制等要\t求。 在工作现场，我们会安装很多的各种类型的传感器，如压力传感器、温度传感器、流扯传感\t器、声音传感器、电参数传感器等。 传感器对环境的适应能力很强，可以应对各种恶劣的工作环\t境。 在日常生活中，如 DV 录像、手机拍照等都属于传感器数据采集的一部分，支持音频、视频、\t图片等文件或附件的采集工作。\t2. 互联网数据\t互联网数据的采集通常借助于网络爬虫来完成。 所谓“网络爬虫＂ （简称爬虫），就是一个在\t159\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 170\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理仁二二二二\t发展起来的，一些经过多年发展的数据采集架构、技术和工具被继承下来，同时，由于大数据本\t身具有数据量大、数据类型丰富、处理速度快等特性，使得大数据采集又表现出不同于传统数据\t采集的一些特点（见表 5-1)。\t表5-1 传统数挹采集与大数括采渠的区别\t传统数据采集 大数据采集\t数据源 来源单一｀数据扯相对较少 来源广泛，数据扯巨大\t数据类型丰富，包括结构化、半结构化和非结构化\t数据类型 结构单一\t数据\t数据存储 关系数据库和并行数据仓库 分布式数据库，分布式文件系统\t5.1.2 数据采集的三大要点\t数据采集的三大要点如下。\t(1) 全面性。 全面性是指数据量足够具有分析价值、数据面足够支撑分析需求。 比如对于“查\t看商品详清”这一行为，需要采集用户触发时的环境信息、会话以及用户 ID, 最后需要统计这一\t行为在某一时段触发的人数、次数、人均次数、活跃比等。\t(2) 多维性。数据更重要的是能满足分析需求。 数据采集必须能够灵活、快速自定义数据的\t多种属性和不同类型，从而满足不同的分析目标要求。 比如“查看商品详情”这一行为，通过“埋\t点”，我们才能知道用户查看的商品是什么、商品价格、商品类型、商品 ID 等多个属性，从而知\t道用户看过哪些商品、什么类型的商品被查看得多、某一个商品被查看了多少次，而不仅是知道\t用户进入了商品详情页。\t(3) 高效性。 高效性包含技术执行的高效性、团队内部成员协同的高效性，以及数据分析需\t求和目标实现的高效性。 也就是说，采集数据一定要明确采集目的，带着问题搜集信息，使信息\t采集更高效、更有针对性。 此外，采集数据还要考虑数据的及时性。\t5.1.3 数据采集的数据源\t数据采集的主要数据源包括传感器数据、互联网数据、日志文件、企业业务系统数据等。\t1. 传感器数据\t传感器是一种检测装置，能感受到被测量环境的信号，并能将感受到的信号按一定规律变换\t成电信号或其他所需形式的信号输出，以满足信息的传输、处理、存储、显示、记录和控制等要\t求。 在工作现场，我们会安装很多的各种类型的传感器，如压力传感器、温度传感器、流扯传感\t器、声音传感器、电参数传感器等。 传感器对环境的适应能力很强，可以应对各种恶劣的工作环\t境。 在日常生活中，如 DV 录像、手机拍照等都属于传感器数据采集的一部分，支持音频、视频、\t图片等文件或附件的采集工作。\t2. 互联网数据\t互联网数据的采集通常借助于网络爬虫来完成。 所谓“网络爬虫＂ （简称爬虫），就是一个在\t159\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 175\n",
      "<class 'str'>\n",
      "二 大数据导论 I\t3. 网络爬虫的类型\t网络爬虫可以分为通用网络爬虫、聚焦网络爬虫、增掀式网络爬虫、深层网络爬虫 4 种类型＾\t(l) 通用网络爬虫。 通用网络爬虫又称“全网爬虫”(Scalable Web Crawler)，抓取对象从一\t些种子 URL 扩充到整个 Web，该类爬虫主要为门户站点搜索引擎和大型 Web 服务提供商采集数\t据。 通用网络爬虫的结构大致可以包括页面抓取模块、页面分析模块、链接过滤模块、页面数据\t库、 URL 队列和初始 URL 集合。 为提高工作效率，通用网络爬虫会采取一定的抓取策略。 常用\t的抓取策略有：深度优先策略和广度优先策略等。\t(2)聚焦网络爬虫。聚焦网络爬虫(Focused Crawler)，又称“主题网络爬虫”(Topical Crawler),\t是指选择性地抓取那些与预先定义好的主题相关的页面的网络爬虫。 和通用网络爬虫相比，聚焦\t网络爬虫只需要抓取与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数鼠\t少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求。 聚焦网络爬虫的T什作流程\t较为复杂，需要根据一定的网页分析算法过滤与主题无关的 URL, 保留有用的 URL 并将其放入\t等待抓取的 URL 队列。 然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页，并重\t复上述过程，直到达到系统的特定条件时停止。 另外，所有被爬虫抓取的网页将会被系统存储，\t进行一定的分析、过滤，并建立索引，以便用于之后的查询和检索。 对于聚焦网络爬虫来说，这\t一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。 聚焦网络爬虫常用的策略包\t括： 基千内容评价的抓取策略、基千链接结构评价的抓取策略、基于增强学习的抓取策略和基于\t语境图的抓取策略等。\t(3) 增撮式网络爬虫。 增釐式网络爬虫 (Incremental Web Crawler) 是指对已下载页面采取增\t釐式更新和只抓取新产生的或者巳经发生变化页面的爬虫，它能够在一定程度上保证所抓取的页\t面是尽可能新的页面。 与周期性抓取和刷新页面的网络爬虫相比，增量式网络爬虫只会在需要的\t时候抓取新产生或发生更新的页面，并不重新下载没有发生变化的页面，可有效减少数据下载队，\t及时更新巳抓取的页面，减小时间和空间上的耗费，但是增加了抓取算法的复杂度和实现难度。\t增量式网络爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面\t的质釐。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容。\t为了实现第二个目标，增拭式爬虫需要对网页的重要性排序，常用的策略包括广度优先策略和\tPageRank 优先策略等。\t(4) 深层网络爬虫。 深层网络爬虫将 Web 页面按存在方式分为表层网页 (Surface Web) 和深\t层网页 (Deep Web, 也称 InvisibleWeb Page 或 Hidden Web)。表层网页是指传统搜索引擎可以索\t引的、超链接可以到达的静态网页。 深层网页是那些大部分内容不能通过静态链接获取的、隐藏\t在搜索表单后的、只有用户提交一些关键词才能获得的 Web 页面。 深层网络爬虫体系结构包含 6\t个基本功能模块（抓取控制器、解析器、表单分析器、表单处理器、响应分析器、 LVS 控制器）\t和两个爬虫内部数据结构 (URL列表、 LVS 表）。\t4. Scrapy 爬虫\tScrapy 是一套基于 Twisted 的异步处理框架，是纯 Python 实现的爬虫框架，用户只需要定制\t164\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 180\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理 仁．~\t(4) 规范化处理：将属性值按比例缩放，使之落入一个特定的区间，比如 0.0......,1.0。 常用的\t数据规范化方法包括 Min-Max 规范化、 Z-Score规范化和小数定标规范化等。\t(5) 属性构造处理：根据已有属性集构造新的属性，后续数据处理直接使用新增的属性。 例\t如，根据已知的质批和体积屈性，计算出新的属性——密度。\t5.3.2 平滑处理\t噪声是指被测变批的一个随机错误和变化。 平滑处理旨在帮助去掉数据中的噪声。 常用的方\t法包括分箱、回归和聚类等。\t1．分箱\t分箱 ( Bin) 利用被平滑数据点的周围点（近邻点），对一组排序数据进行平滑处理，排序后\t的数据被分配到若干箱子中。\t典型的分箱方法一般有两种： 一种是等高方法，即每个箱子中元素的个数相等；另一种是等\t宽方法，即每个箱子的取值间距（左右边界之差）相同，如图 5-6 所示。\t箱子中元素的个数 箱子中元素的个数\t(a)等高方法 属性值 (b)等宽方法 属性值\t图 5-6 两种典型分箱方法\t这里给出一个实例介绍分箱。 假设有一个数据集X={4,8,15,21,21,24,25,28,34} ，这里采用基于\t平均值的等高方法对其进行平滑处理，则分箱处理的步骤如下。\t(1) 把原始数据集 X放入以下 3 个箱子。\t箱子 1: 4,8,15。\t箱子 2: 21,21,24。\t箱子 3: 25,28,34。\t(2) 分别计算每个箱子的平均值。\t箱子 1 的平均值： 9。\t箱子 2 的平均值： 22。\t箱子 3 的平均值： 29。\t(3) 用每个箱子的平均值替换该箱子内的所有元素。\t箱子 1 : 9,9,9。\t箱子 2: 22,22,22。\t箱子 3: 29,29,29。\t(4) 合并各个箱子中的元素得到新的数据集{9,9,9,22,22,22,29,29必9}。\t169\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 123\n",
      "<class 'str'>\n",
      "＝ 大数据导论 |\t为用户提供定制化的数据服务，由于需要涉及数据的处理加工，因此，该类型平台的业务相对复\t杂，国内大数据交易平台大多属于这种类型 而第三方数据交易平台业务相对简单明确，主要负\t责对交易过程的监管，通常可以提供数据出售、数据购买、数据供应方查询以及数据需求发布等\t服务。\t此外， 从大数据交易平台的建设与运营主体角度来说，目前的大数据交易平台还可以划分为\t3 种类型： 政府主导的大数据交易平台、企业以市场需求为导向建立的大数据交易平台、产业联\t盟性质的大数据交易平台。 其中，产业联盟性质的大数据交易平台（如中关村大数据产业联盟、\t中国大数据产业联盟、上海大数据产业联盟），侧亟于数据的共享，而不是数据的交易，\t2. 交易平台的数据来源\t交易平台的数据来源主要包括政府公开数据、企业内部数据、数据供应方数据、 网页爬虫数\t据等。\t(1) 政府公开数据。 政府数据资源开放共享是世界各国实施大数据发展战略的重要举措。 政\t府作为公共数据的核心生产者和拥有者，汇集了最具挖掘价值的数据资源。 加快政府数据开放共\t享，释放政府数据和机构数据的价值．对大数据交易市场的繁荣将起到重要作用。\t(2) 企业内部数据。 企业在生产经营过程中，积累了海批的数据，包括产品数据、设备数据、\t研发数据、供应链数据、运营数据、管理数据、销售数据、消费者数据等，这些数据经过处理加\t工以后，是具有重要商业价值的数据源。\t(3) 数据供应方数据。 该类型的数据一般是由数据供应方在数据交易平台上根据交易平台的\t规则和流程提供自己所拥有的数据。\t(4) 网页爬虫数据。 通过相关技术手段，从全球范围内的互联网网页爬取的数据。\t多种数据来源渠道可以使得交易平台数据更加丰富，但是，同时增加了数据监管难度。 在 IT\t飞速发展的时代，信息收集变得更加容易，信息滥用、个人数据倒卖情况屡见不鲜，因此，在数\t据来源广泛的情况下，更要加强对交易平台的安全监管。\t3. 交易平台的产品类型\t不同的交易平台会根据自己的目标和定位， 提供不同的产品类型，用户可以根据自己的个性\t化需求合理地选择交易平台。 交易产品的类型主要有以下几种 ： API、数据包、云服务、解决方\t案、数据定制服务以及数据产品。\t( 1) APl。 API 是应用程序接口，数据供应方对外提供数据访间接口 ， 数据需求方直接通过调\t用接口来获得所需的数据。\t(2) 数据包。 数据包的数据，既可以是未经处理的原始数据，也可以是经过加工处理以后的\t数据。\t(3) 云服务。 云服务是在云计算不断发展的背景下产生的，通常通过互联网来提供实时的、\t动态的资源。\t(4) 解决方案。 在特定的情景下， 利用已有的数据，为需求方提供处理问题的方案，比如数\t据分析报告等。\t112\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 181\n",
      "<class 'str'>\n",
      "＝＝寸大数据导论 I\t此外，还可以采用基于箱子边界的等高方法对数据进行平滑处理。 利用边界进行平滑处理时，\t对于给定的箱子，其最大值与最小值就构成了该箱子的边界，利用每个箱子的边界值（最大值或\t最小值）替换该箱子中的所有值。 这时的分箱结果如下。\t箱子 1: 4,4,15。\t箱子 2: 21,21,24。\t箱子 3: 25,25,34。\t合并各个箱子中的元素得到新的数据集{4,4, 15,21,21,24,25,25,34}。\t2. 回归\t回归是利用拟合函数对数据进行平滑处理。例如，借助线性回归方法（包括多变量回归方\t法），可以获得多个变量之间的拟合关系，从而达到利用一个（或一组）变量值来预测另一个变\t量值的目的。利用线性回归方法所获得的拟合函数，能够平滑数据并除去其中的噪声3 对数据进\t行线性回归拟合如图 5-7 所示。\t上网时长时序分析\t--总时长仍小时） 多硕式（总时长历小时））\t12.00\t了： \\\t-C、节\t6.00\t4.00\t2.00\t0.00\t1124 1'126 1128 1130 1202 1204 1206 1208 1210 1212\t图 5-7 对数据进行线性回归拟合\t3. 聚类\t聚类可帮助发现异常数据。 如图 5-8 所示，相似或相邻的数据聚合在一起形成了各个聚类集\t合，而那些位于这些聚类集合之外的数据对象，被认为是异常数据。\t.\t.\t.\t+. .\t.\t.\t.\t图 5-8 基千聚类的异常数据监测\t170\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 180\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理 仁．~\t(4) 规范化处理：将属性值按比例缩放，使之落入一个特定的区间，比如 0.0......,1.0。 常用的\t数据规范化方法包括 Min-Max 规范化、 Z-Score规范化和小数定标规范化等。\t(5) 属性构造处理：根据已有属性集构造新的属性，后续数据处理直接使用新增的属性。 例\t如，根据已知的质批和体积屈性，计算出新的属性——密度。\t5.3.2 平滑处理\t噪声是指被测变批的一个随机错误和变化。 平滑处理旨在帮助去掉数据中的噪声。 常用的方\t法包括分箱、回归和聚类等。\t1．分箱\t分箱 ( Bin) 利用被平滑数据点的周围点（近邻点），对一组排序数据进行平滑处理，排序后\t的数据被分配到若干箱子中。\t典型的分箱方法一般有两种： 一种是等高方法，即每个箱子中元素的个数相等；另一种是等\t宽方法，即每个箱子的取值间距（左右边界之差）相同，如图 5-6 所示。\t箱子中元素的个数 箱子中元素的个数\t(a)等高方法 属性值 (b)等宽方法 属性值\t图 5-6 两种典型分箱方法\t这里给出一个实例介绍分箱。 假设有一个数据集X={4,8,15,21,21,24,25,28,34} ，这里采用基于\t平均值的等高方法对其进行平滑处理，则分箱处理的步骤如下。\t(1) 把原始数据集 X放入以下 3 个箱子。\t箱子 1: 4,8,15。\t箱子 2: 21,21,24。\t箱子 3: 25,28,34。\t(2) 分别计算每个箱子的平均值。\t箱子 1 的平均值： 9。\t箱子 2 的平均值： 22。\t箱子 3 的平均值： 29。\t(3) 用每个箱子的平均值替换该箱子内的所有元素。\t箱子 1 : 9,9,9。\t箱子 2: 22,22,22。\t箱子 3: 29,29,29。\t(4) 合并各个箱子中的元素得到新的数据集{9,9,9,22,22,22,29,29必9}。\t169\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 182\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理仁二二＝二\t5.3.3 规范化处理\t规范化处理是一种重要的数据转换策略。 它将一个属性取值范闱投射到一个特定范围，以消\t除数值型属性因大小不一而造成挖掘结果的偏差，常用于神经网络、基于距离计算的最近邻分类\t和聚类挖掘的数据预处理等。 对于神经网络，采用规范化后的数据，不仅有助于确保学习结果的\t正确性，而且会帮助提高学习的效率。 对于基于距离计算的挖掘，规范化方法可以帮助消除因属\t性取值范围不同而影响挖掘结果的公正性的情况。\t常用的规范化处理方法包括 Min-Max规范化、 Z-Score规范化和小数定标规范化。\t1. Min-Max 规范化\tMin-Max 规范化方法对被转换数据进行一种线性转换，其转换公式如下：\tx=（待转换属性值－属性最小值）／（属性最大值－属性最小值）\t例如，假设属性的最大值和最小值分别是 87 000 元和 11 000 元，现在需要利用 Min-Max 规\t范化方法，将“顾客收入“属性的值映射到 0~1 的范圉内，则”顾客收入“属性的值为 72400\t元时，对应的转换结果如下：\t(72400-1 I 000)/(87000-1 I 000);::;0.808\tMin-Max 规范化比较简单，但是也存在一些缺陷，当有新的数据加入时，可能导致最大值和\t最小值的变化，需要重新定义属性最大值和最小值。\t2. Z-Score 规范化\tZ-Score 规范化的主要目的是将不同量级的数据统一转化为同一个量级的数据，统一用计算出\t的 Z-Score 值衡掀，以保证数据之间的可比性。 其转换公式如下：\tz=（待转换属性值－属性平均值）／属性标准差\t假设我们要比较学生 A 与学生 B 的考试成绩， A 的考卷满分是 100 分（及格 60 分）， B 的考\t卷满分是 700 分（及格 420 分）。 很显然， A 考出的 70 分与 B 考出的 70 分代表着完全不同的意\t义。 但是从数值来讲， A 与 B 在数据表中都是用数字 70 代表各自的成绩。 那么如何能够用一个\t同等的标准来比较 A 与 B 的成绩呢？ Z-Score 就可以解决这一问题。\t假设 A 班级的平均分是 80，标准差是 10, A考了 90 分； B 班的平均分是 400，标准差是 100,\tB 考了 600 分。 通过上面的公式，我们可以计算得出， A 的 Z-Score 是 1 （即（90-80)/10), B 的\tZ-Score 是 2 （即（600-400)/100)，因此， B 的成绩更为优异。 若 A 考了 60 分， B 考了 300 分，则\tA 的 Z-Score 是－2, B 的 Z-Score是－l, A 的成绩比较差。\tZ-Score 的优点是不需要知道数据集的最大值和最小值，对离群点规范化效果好。 此外，\tZ-Score 能够应用于数值型的数据，并且不受数据盐级的影响，因为它本身的作用就是消除釐级给\t分析带来的不便。\t但是 Z-Score 也有一些缺陷。 首先， Z-Score 对于数据的分布有一定的要求，正态分布是最有\t利于 Z-Score 计算的。 其次， Z-Score 消除了数据具有的实际意义， A 的 Z-Score 与 B 的 Z-Score\t与他们各自的分数不再有关系，因此， Z-Score 的结果只能用于比较数据间的结果，探究数据的真\t171\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 183\n",
      "<class 'str'>\n",
      "＝大数据导论 1\t实意义时还需要还原数据。\t3. 小数定标规范化\t小数定标规范化通过移动属性值的小数位置来达到规范化的目的。 所移动的小数位数取决于\t属性绝对值的最大值。 其转换公式为 ：\t已寺转换属性值／10k\t其中， k为能够使该属性绝对值的最大值的转换结果小于 1 的最小值。\t比如，假设属性的取值范围是－957~924，则该属性绝对值的最大值为 957, 很显然，这时\tk=3。 当属性的值为 426 时，对应的转换结果如下：\t426/103 = 0.426\t小数定标规范化的优点是直观简单，缺点是并没有消除属性间的权重差异。\t5.4\t数据脱敏\t数据脱敏是在给定的规则、策略下对敏感数据进行变换、修改的技术，能够在很大程度上解\t决敏感数据在非可信环境中使用的问题。 它会根据数据保护规范和脱敏策略，通过对业务数据中\t的敏感信息实施自动变形，实现对敏感信息的隐藏和保护。 在涉及客户安全数据或者一些商业性\t敏感数据的情况下，在不违反系统规则的条件下，需对身份证号、手机号、银行卡号、客户号等\t个人信息进行数据脱敏。 数据脱敏不是必需的数据预处理环节，可以根据业务需求对数据进行脱\t敏处理，也可以不进行脱敏处理。\t5.4.1 数据脱敏原则\t数据脱敏不仅需要执行“数据漂白＂，抹去数据中的敏感内容，同时需要保持原有的数据特征、\t业务规则和数据关联性，保证开发、测试以及大数据类业务不会受到脱敏的影响，达成脱敏前后\t的数据一致性和有效性，具体如下。\t(1) 保持原有数据特征。 数据脱敏前后必须保持原有数据特征，例如：身份证号码由 17 位数\t字本体码和 1 位校验码组成，分别为区域地址码 (6 位）、出生日期码 (8 位）、顺序码 (3 位）和\t校验码 (1 位）。 那么身份证号码的脱敏规则就需要保证脱敏后依旧保持这些特征信息。 ·\t(2) 保持数据之间的一致性。 在不同业务中，数据和数据之间具有一定的关联性。 例如：出\t生年月或出生日期和年龄之间的关系。 同样，身份证信息脱敏后仍需要保证出生日期字段和身份\t证中包含的出生日期之间的一致性。\t(3) 保持业务规则的关联性。 保持数据业务规则的关联性是指数据脱敏时数据关联性和业务\t语义等保持不变，其中数据关联性包括：主外键关联性、关联字段的业务语义关联性等。 特别是\t高度敏感的账户类主体数据，往往会贯穿主体的所有关系和行为信息， 因此需要特别注意保证所\t有相关主体信息的一致性。\t172\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 179\n",
      "<class 'str'>\n",
      "二 大数据导论 l\t要对二者的数据类型进行统一处理。\t4. 重复值处理\t重复值的存在会影响数据分析和挖掘结果的准确性，所以，在数据分析和建模之前需要进行\t数据重复性检验，如果存在重复值，还需要删除重复值。\t5.2.2 数据清洗的注意事项\t在进行数据清洗时，需要注意如下事项。\t( 1 ) 数据清洗时可优先进行缺失值、异常值和数据类型转换的操作，最后进行重复值处理\t(2) 在对缺失值、异常值进行处理时，要根据业务的需求进行处理，这些处理并不是一成不\t变的。 常见的填充包括：统计值填充（常用的统计值有均值、 中位数、 众数）、前／后值填充（ 一\t般在前后数据存在关联时使用，比如数据是按照时间进行记录的）、零值填充。\t(3) 在数据清洗之前，最重要的是对数据表进行查看，要了解表的结构和发现需要处理的值，\t才能将数据清洗彻底。\t( 4) 数据扯的大小也关系着数据的处理方式。 如果总数据批较大，而异常的数据（包括缺失\t值和异常值）的最较少时，可以选择直接删除，因为这通常并不太会影响到最终的分析结果；但\t是，如果总数据拯较小， 则每个数据都可能影响分析结果，这个时候就需要认真去对数据进行处\t理（可能需要通过其他的关联表去找到相关数据进行填充）。\t( 5 ) 在导入数据表后， 一般需要对所有列依次地进行清洗，来保证数据处理的彻底性。 有些\t数据可能看起来是正常可以使用的， 实际上在进行处理时可能会出现问题（比如某列数据在查看\t时看起来是数值类型，但是其实这列数据是字符串类型，这就会导致在进行数值操作时无法使用）。\t5.3\t数据转换\t数据转换就是将数据进行转换或归并，从而构成一个适合数据处理的形式。 本节首先介绍常\t见的数据转换策略，然后重点介绍数据转换策略中的平滑处理和规范化处理。\t5.3.1 数据转换策略\t常见的数据转换策略如下。\t( 1 ) 平滑处理：帮助除去数据中的噪声。 常用的方法包括分箱、 回归和聚类等。\t(2) 聚集处理： 对数据进行汇总操作。 例如，每天的数据经过汇总操作可以获得每月或每年\t的总额。 这一操作常用于构造数据立方体或对数据进行多粒度的分析。\t(3) 数据泛化处理：用更抽象（更高层次）的概念来取代低层次的数据对象。 例如，街道属\t性可以泛化到更高层次的概念，如城市、 国家，再比如年龄属性可以映射到更高层次的概念，如\t青年、 中年和老年。\t168\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 182\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理仁二二＝二\t5.3.3 规范化处理\t规范化处理是一种重要的数据转换策略。 它将一个属性取值范闱投射到一个特定范围，以消\t除数值型属性因大小不一而造成挖掘结果的偏差，常用于神经网络、基于距离计算的最近邻分类\t和聚类挖掘的数据预处理等。 对于神经网络，采用规范化后的数据，不仅有助于确保学习结果的\t正确性，而且会帮助提高学习的效率。 对于基于距离计算的挖掘，规范化方法可以帮助消除因属\t性取值范围不同而影响挖掘结果的公正性的情况。\t常用的规范化处理方法包括 Min-Max规范化、 Z-Score规范化和小数定标规范化。\t1. Min-Max 规范化\tMin-Max 规范化方法对被转换数据进行一种线性转换，其转换公式如下：\tx=（待转换属性值－属性最小值）／（属性最大值－属性最小值）\t例如，假设属性的最大值和最小值分别是 87 000 元和 11 000 元，现在需要利用 Min-Max 规\t范化方法，将“顾客收入“属性的值映射到 0~1 的范圉内，则”顾客收入“属性的值为 72400\t元时，对应的转换结果如下：\t(72400-1 I 000)/(87000-1 I 000);::;0.808\tMin-Max 规范化比较简单，但是也存在一些缺陷，当有新的数据加入时，可能导致最大值和\t最小值的变化，需要重新定义属性最大值和最小值。\t2. Z-Score 规范化\tZ-Score 规范化的主要目的是将不同量级的数据统一转化为同一个量级的数据，统一用计算出\t的 Z-Score 值衡掀，以保证数据之间的可比性。 其转换公式如下：\tz=（待转换属性值－属性平均值）／属性标准差\t假设我们要比较学生 A 与学生 B 的考试成绩， A 的考卷满分是 100 分（及格 60 分）， B 的考\t卷满分是 700 分（及格 420 分）。 很显然， A 考出的 70 分与 B 考出的 70 分代表着完全不同的意\t义。 但是从数值来讲， A 与 B 在数据表中都是用数字 70 代表各自的成绩。 那么如何能够用一个\t同等的标准来比较 A 与 B 的成绩呢？ Z-Score 就可以解决这一问题。\t假设 A 班级的平均分是 80，标准差是 10, A考了 90 分； B 班的平均分是 400，标准差是 100,\tB 考了 600 分。 通过上面的公式，我们可以计算得出， A 的 Z-Score 是 1 （即（90-80)/10), B 的\tZ-Score 是 2 （即（600-400)/100)，因此， B 的成绩更为优异。 若 A 考了 60 分， B 考了 300 分，则\tA 的 Z-Score 是－2, B 的 Z-Score是－l, A 的成绩比较差。\tZ-Score 的优点是不需要知道数据集的最大值和最小值，对离群点规范化效果好。 此外，\tZ-Score 能够应用于数值型的数据，并且不受数据盐级的影响，因为它本身的作用就是消除釐级给\t分析带来的不便。\t但是 Z-Score 也有一些缺陷。 首先， Z-Score 对于数据的分布有一定的要求，正态分布是最有\t利于 Z-Score 计算的。 其次， Z-Score 消除了数据具有的实际意义， A 的 Z-Score 与 B 的 Z-Score\t与他们各自的分数不再有关系，因此， Z-Score 的结果只能用于比较数据间的结果，探究数据的真\t171\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 183\n",
      "<class 'str'>\n",
      "＝大数据导论 1\t实意义时还需要还原数据。\t3. 小数定标规范化\t小数定标规范化通过移动属性值的小数位置来达到规范化的目的。 所移动的小数位数取决于\t属性绝对值的最大值。 其转换公式为 ：\t已寺转换属性值／10k\t其中， k为能够使该属性绝对值的最大值的转换结果小于 1 的最小值。\t比如，假设属性的取值范围是－957~924，则该属性绝对值的最大值为 957, 很显然，这时\tk=3。 当属性的值为 426 时，对应的转换结果如下：\t426/103 = 0.426\t小数定标规范化的优点是直观简单，缺点是并没有消除属性间的权重差异。\t5.4\t数据脱敏\t数据脱敏是在给定的规则、策略下对敏感数据进行变换、修改的技术，能够在很大程度上解\t决敏感数据在非可信环境中使用的问题。 它会根据数据保护规范和脱敏策略，通过对业务数据中\t的敏感信息实施自动变形，实现对敏感信息的隐藏和保护。 在涉及客户安全数据或者一些商业性\t敏感数据的情况下，在不违反系统规则的条件下，需对身份证号、手机号、银行卡号、客户号等\t个人信息进行数据脱敏。 数据脱敏不是必需的数据预处理环节，可以根据业务需求对数据进行脱\t敏处理，也可以不进行脱敏处理。\t5.4.1 数据脱敏原则\t数据脱敏不仅需要执行“数据漂白＂，抹去数据中的敏感内容，同时需要保持原有的数据特征、\t业务规则和数据关联性，保证开发、测试以及大数据类业务不会受到脱敏的影响，达成脱敏前后\t的数据一致性和有效性，具体如下。\t(1) 保持原有数据特征。 数据脱敏前后必须保持原有数据特征，例如：身份证号码由 17 位数\t字本体码和 1 位校验码组成，分别为区域地址码 (6 位）、出生日期码 (8 位）、顺序码 (3 位）和\t校验码 (1 位）。 那么身份证号码的脱敏规则就需要保证脱敏后依旧保持这些特征信息。 ·\t(2) 保持数据之间的一致性。 在不同业务中，数据和数据之间具有一定的关联性。 例如：出\t生年月或出生日期和年龄之间的关系。 同样，身份证信息脱敏后仍需要保证出生日期字段和身份\t证中包含的出生日期之间的一致性。\t(3) 保持业务规则的关联性。 保持数据业务规则的关联性是指数据脱敏时数据关联性和业务\t语义等保持不变，其中数据关联性包括：主外键关联性、关联字段的业务语义关联性等。 特别是\t高度敏感的账户类主体数据，往往会贯穿主体的所有关系和行为信息， 因此需要特别注意保证所\t有相关主体信息的一致性。\t172\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 184\n",
      "<class 'str'>\n",
      "1 第 5章数据采集与预处理 |~\t(4) 多次脱敏数据之间的数据一致性。 对相同的数据进行多次脱敏，或者在不同的测试系统\t进行脱敏，需要确保每次脱敏的数据始终保持一致性，只有这样才能保障业务系统数据变更的持\t续一致性和广义业务的持续一致性。\t5.4.2 数据脱敏方法\t数据脱敏主要包括以下方法。\t(I) 数据替换。 用设置的固定虚构值替换真值。 例如将手机号码统一替换为 139***10002。\t(2) 无效化。 通过对数据值的截断、加密、隐藏等方式使敏感数据脱敏，使其不再具有使用\t价值，例如将地址的值替换为“*＊＊＊＊＊＂。 数据无效化与数据替换所达成的效果基本类似。\t(3) 随机化。 采用随机数据代替真值，保持替换值的随机性以模拟样本的真实性。 例如用随\t机生成的姓和名代替真值。\t(4) 偏移和取整。 通过随机移位改变数字数据，例如把日期 “2018-01-02 8:12:25\" 变为\t\"2018-01-02 8:00:00\"。 偏移取整在保持了数据的安全性的同时，保证了范围的大致真实性，此项\t功能在大数据利用环境中具有重大价值。\t(5) 掩码屏蔽。 掩码屏蔽是针对账户类数据的部分信息进行脱敏时的有力工具，比如对银行卡号\t或是身份证号的脱敏。 例如，把身份证号码 ”220524199?09010254\" 替换为 “220524********0254”。\t(6) 灵活编码。 在需要特殊脱敏规则时，可执行灵活编码以满足各种可能的脱敏规则。 例如\t川固定字母和固定位数的数字替代合同编号真值。\t5.5\t本章小结\t数据采集与预处理是大数据分析全流程的关键一环，直接决定了后续环节分析结果的质矗高\t低C 近年来，以大数据、物联网、人T智能、 5G 为核心特征的数字化浪潮正席卷全球。 随着网络\t和信息技术的不断普及，人类产生的数据量正在呈指数级增长，大约每两年翻一番，这意味着人\t类在最近两年产生的数据扯相当于之前产生的全部数据扯。世界上每时每刻都在产生大撮的数据，\t包括物联网传感器数据、社交网络数据、企业业务系统数据等。 面对如此海扯的数据，如何有效\t收集这些数据并且进行清洗、转换，已经成为巨大的挑战。 因此需要用相关的技术来收集数据，\t并且对数据进清洗、转换和脱敏。\t本章介绍了数据采集、数据清洗、数据转换和数据脱敏的方法。\t5.6\t习题\t1. 请阐述传统数据采集与大数据采集的区别。\t173\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 271\n",
      "<class 'str'>\n",
      "＝＝才 大数据导论 l\t例如，将用户 (User) 对物品 (Item) 的评分矩阵分韶为两个矩阵：一个是用户对物品隐含\t特征的偏好矩阵，另一个是物品所包含的隐含特征的矩阵。 具体而言，将“用户－物品”的评分\t矩阵 R 分解成两个隐含因子矩阵 P 和 Q, 从而将用户和物品都投影到一个隐含因子的空间中去\t即对于 R(mxn)的矩阵， ALS 旨在找到两个低维矩阵 P(mxk)和矩阵 Q(nxk)，来近似逼近R(mxn):\tRmxn pmxkQ，了xk （式 9-1 )\tc::::\t其中， k<＜血n(m, n)。 这里相当于降维了，矩阵 P和 Q也被称为低秩矩阵。 需要注意的是，\t我们并不需要显式地定义这 k个关联维度，而只需要假定它们存在即可，因此这里的关联维度又\t被称为“隐语义因子”(Latent Factor), k 的典型取值一般是 20~200。 这种方法被称为”概率矩\t阵分解算法”(Probabilistic Matrix Factorization, PMF)。 ALS 算法是 PMF 在数值计算方面的应用J\t这里简要说明 ALS 的低秩假设为什么是合理的。 大千世界，人海茫茫，人们的喜好各不相同\t描述一个人的喜好，通常都可以在一个抽象的低维空间上进行，并不需要将其喜欢的事物一一列\t出。 比如，用户 a 喜欢看战争题材的电影，则根据这个描述就知道用户 a 大概会喜欢 《集结号》\t和《拯救大兵瑞恩》等电影，因为这些电影都符合用户 a 对自己喜好的描述，也就是说，这些电\t影在这个抽象的低维空间的投影和用户 a 的喜好相似。 进一步地，可以把用户的喜好和电影的特\t征都投影到一个低维空间，比如，把一个用户的喜好投影到一个低维向址p,，把一个电影的特征\t投影到一个低维向量 qj，那么这个用户和这个电影的相似度就可以表示成这两个向址之间的内积\tp矿。 如果把评分理解成相似度，那么矩阵A 就可以由用户喜好矩阵 P 和电影特征矩阵 Q 的乘积\tPQT来近似了\t为了使低秩矩阵 P 和 Q尽可能地逼近 R, 可以最小化下面的损失函数L 来完成：\tl(P,Q) ＝又（片，， －Puq厅＋A（| pu |2 +|q1 广） （式 9-2)\t其中 ， Pu表示用户 u 的偏好的隐含特征向址， q,表示物品 I 包含的隐含特征向址， ru，表示用\t户 u 对物品 i 的评分，向址p,，和 q，的内积 Puq} 是用户 u 对物品 i评分的近似。 最小化该损失函数\t使得两个隐语义因子矩阵的乘积尽可能逼近原始的评分。 同时，损失函数中增加了 L2 规范化项\t(Regularization Term)，对较大的参数值进行惩罚，以减小过拟合造成的影响J\tALS 是求解 L(P. Q)的著名算法，它的基本思想是：固定其中一类参数，使问题变为单类变\t批优化问题，利用解析方法进行优化；再反过来，固定先前优化过的参数，再优化另一组参数；\t此过程迭代进行， 直到收敛。 ALS 算法中的 “A\" 即“最小交替二乘法”中的“交替＂，就是指我\t们先随机生成 Po, 然后固定 P。去求解 Qo, 再固定 Q。求解 Pl ，这样交替进行下去。 因为每步迭代\t都会降低重构误差，并且误差是有下界的，所以 ALS 一定会收敛。 具体求解过程如下。\tBL(P,Q)\t( l) 固定 Q，对Pu求偏导数 ＝ 0, 得到求解Pu的公式：\t视\tP11 =(QTQ＋入I）一IQTr11 （式 9-3)\tBL(P,Q)\t(2) 固定 P, 对 q,求偏导数 ＝0, 得到求解 q，的公式：\t6q1\tqi =(pTp ＋A,]）一lpTyi （式 9-4)\t260\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 54\n",
      "<class 'str'>\n",
      "l 第 2 章大数据与其他新兴技术的关系仁＝＝二~\t现在非常热门的深度学习 (Deep Leaming) 是机器学习的子类。 它的灵感源于人类大脑的工\t作方式，是利用深度神经网络来解决特征表达的一种学习过程。 深度神经网络本身并非是一个全\t新的概念，可理解为包含多个隐含层的神经网络结构。 为了提高深层神经网络的训练效果，人们\t对神经元的连接方法和激活函数等方面做出了调整。 深度学习的目的在千建立、模拟人脑进行分\t析学习的神经网络，模仿人脑的机制来解释数据，如文本、图像、声音等。\t2. 知识图谱\t知识图谱 (Knowledge Graph) 又称为科学知识图谱，在图书情报界称为知识域可视化或知识\t领域映射地图，是显示知识发展进程与结构关系的一系列各种不同的图形。知识图谱用可视化技\t术描述知识资源及其载体，挖掘、分析、构建、绘制和显示知识及它们之间的相互联系。\t现实世界中的很多场景非常适合用知识图谱来表达。 比如，如图 2-14所示，一个社交网络知\t识图谱里，我们既可以有“人”的实体，也可以包含“公司”实体。 人和人之间可以是＂朋友”,\t也可以是“同事”关系。 人和公司之间可以是“现任职”或者”曾任职“关系。 类似地，一个风\t控知识图谱可以包含“电话”“公司”等实体，电话和电话之间可以是“通话“关系，而且每个公\t司通常会有固定的电话。\t朋友\t公\t司电话\t现任职于\t。\t。\t案例：社交网络知识图谱 案例：风控知识图谱\t图 2-14 知识图谱案例\t知识图谱可用于反欺诈、不一致性验证等公共安全保障领域，需要用到异常分析、静态分析、\t动态分析等数据挖掘方法。 特别地，知识图谱在搜索引擎、可视化展示和精准营销方面有很大\t的优势，已成为业界的热门工具。 但是，知识图谱的发展还有很大的挑战，如数据的噪声问题，\t即数据本身有错误或者数据存在冗余。 随着知识图谱应用的不断深入，还有一系列关键技术需\t要突破。\t3. 自然语言处理\t自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能使人与计算机\t之间用自然语言进行有效沟通的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、\t数学于一体的科学。 因此，这一方向的研究将涉及自然语言，即人们日常使用的语言。 虽然它与\t语言学的研究有着密切的联系，但有重要的区别。自然语言处理并不是一般地研究自然语言，而\t是研制能有效地实现自然语言通信的计算机系统，特别是软件系统。\t43\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 213\n",
      "<class 'str'>\n",
      "： 大数据导论 l\t(2) 在作用层面。 数据分析主要实现三大作用：现状分析、原因分析、预测分析（定址）数据\t分析的目标明确，先做假设，然后通过数据分析来验证假设是否正确，从而得到相应的结论数据挖\t掘主要侧重解决四类问题：分类、聚类、关联和预测（定址、定性）。 数据挖掘的亟点在于寻找术知\t的模式与规律；如著名的数据挖掘案例 啤酒与尿布，就是事先未知的，但又是非常有价值的信息。\t(3) 在方法层面。 数据分析主要采用对比分析、分组分析、交叉分析、回归分析等常用分析\t方法；数据挖掘主要采用决策树、神经网络、关联规则、聚类分析等统计学、人工智能、机器学\t习等方法进行挖掘。\t(4) 在结果层面。 数据分析一般都是得到一个指标统计扯结果，如总和、平均值等，这些数\t据都需要与业务结合进行解读，才能发挥出数据的价值与作用 ） 数据挖掘则是输出模型或规则，\t并且可相应得到模型得分或标签。 模型得分如流失概率值、总和得分、相似度、预测值等，标签\t如高中低价值用户、流失与非流失、信用优良中差等。\t7.1.2 数据分析与数据处理\t数据分析过程通常会伴随着数据处理的发生（或者说伴随着大址数据计算），因此，数据分析\t和数据处理是一对关系紧密的概念，很多时候，二者是融合在一起的，很难割裂开来， 也就是说，\t当用户在进行数据分析的时候，底层的计算机系统会根据数据分析任务的要求，使用程序进行大\t扯的数据处理（或者说发生大批的数据计算）。 例如，当用户进行决策树分析时，需要事先根据决\t策树算法编写分析程序，当分析开始以后，决策树分析程序就会从磁盘读取数据进行大拭计符，\t最终给出计算结果（也就是决策树分析结果）。\t7.1.3 大数据处理与分析\t数据分析包含两个要素，即理论和技术。 在理论层面，需要统计学、机器学习和数据挖掘等\t知识； 在技术层面，包括单机分析工具（如 SPSS、 SAS 等）或单机编程语言（如 Python、 R ),\t以及大数据处理与分析技术（如 MapReduce、 Spark、 Hive 等）。\t数据分析可以是针对小规模数据的分析，也可以是针对大规模数据的分析（这时被称为“大\t数据分析”为在大数据时代到来之前，数据分析主要以小规模的抽样数据为主， 一般使川统计学、\t机器学习和数据挖掘的相关方法，以单机分析工具或者单机编程的方式来实现分析程序。 但是，\t到了大数据时代， 数据址爆炸式地增长，很多时候需要对规模巨大的全蚊数据而不是小规模的抽\t样数据进行分析。 这时，单机下具和单机程序显得“无能为力”，就需要采用分布式实现技术， 比\t如使用 MapReduce、 Spark 或 Flink 编写分布式分析程序，借助于集群的多台机器进行并行数据处\t理分析，这个过程就被称为“大数据处理与分析”。\t本章后续内容中，在数据分析理论层面，只介绍关于数据挖掘的理论知识（即机器学习和数据挖\t掘算法），对于使用统计学方法的狭义的数据分析理论知识（如对比分析、分组分析、交叉分析、预\t测分析、漏斗分析、 A/B 测试分析、结构分析、因素分析、矩阵分析、相关分析、回们分析、聚类分\t析、判断分析、成分分析等）不做介绍，感兴趣的读者可以参考相关的统计学书籍。 在数据分析技术\t202\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 269\n",
      "<class 'str'>\n",
      "二大数据导论 I\t亲爱的用户：hadoop，猜你喜欢电影：\tAnne Frank Remembered (19'巧）\tMcCabe & M~. Miller(1971) Smashing Time (1967) Man of the (en血y(1吵9,\tt.N Ju, 谣咄翩\tw\"才” c炽丸\t飞产”`c, `. l It\tri<••· MILLEll\t尺\t也\t'\t”\t}\t`· 叩飞l, I\t图 9-5 电影推荐结果页面\t9.2.4 算法设计\t1. 算法的选择\t电影推荐程序的设计是电影推荐系统设计的核心。 电影推荐程序需要完成推荐功能，因此，\t我们可以选择经典的推荐算法——协同过滤算法。 基千 ALS 矩阵分解的协同过滤算法（简称 ASL\t算法）就是典型的基于模型的协同过滤算法。 基于模型的协同过滤算法是通过巳经观察到的所有\t用户给产品的打分，来推断每个用户的喜好并向用户推荐适合的产品。 本系统就采用该算法来预\t测某个用户对某部电影的评分，并把合适的电影推荐给该用户。\t2. ALS 算法的基本原理\t在实际应用中，用户和商品的关系可以抽象为一个三元组<Userjtem,Rating>，其中， User表\t示用户， Item 表示物品， Rating 表示用户对物品的评分，即用户对物品的喜好程度。\t用户对物品的评分行为可以表示成一个“用户－物品”矩阵A(u*v)，该矩阵表示 u 个用户对\tv个物品的评分情况。 表 9-1 是一个评分矩阵实例。\t表9-1 用户对物昂的钾分\tVI V2 V3 V4 V5\tUt 3 5 4 ? I\t4 ? 3 3 l\tU2\tU3 3 4 5 3 2\tU4 4 4 3 2 I\t2 4 ? I 2\tU5\t? 5 4 l 2\tU6\t其中，矩阵 A 的每个元素 Alj，表示用户 u1对物品 v;的评分。 由千用户不会对所有的物品都\t进行评分，因此，在矩阵A 中会不可避免地存在一些“缺失值＂ （表 9-1 中用问号标记的地方），\t这意味着 A 是一个稀疏矩阵，里面会存在很多空值（见图 9-6)。 基于模型的协同过滤算法就是根\t据已经观察到的用户、物品信息来预测矩阵A 中的＂缺失值”。\t258\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 277\n",
      "<class 'str'>\n",
      "＝大数据导论 l\t[53] 谢志燕．以思维方式的转变拥抱大数据时代的到来[J] 安徽冶金科技职业学院学报，\t2018(03): 107-109.\t[54] 何翼，田华．从＂棱镜门”事件领略大数据时代[J]．福建电脑， 2016(1):39-39\t[55] 孙悦新大数据时代我国国家安全治理的风险化解［J]. 经贸实践， 2018(22):219\t[56] 陈仕伟大数据时代隐私保护的伦理反思[J]．甘肃行政学院学报， 2018(6):104-112\t[57] 赵丁．大数据云计算语境下的数据安全应对策略[J]．电子技术与软件丁程， 2019(2):210.\t[58] 杨继武关于大数据时代下的网络安全与隐私保护探析[J]．通讯世界， 2019(2):35-36.\t[59] 孙得，王镜涵互联网大数据时代对国家安全影响［J]. 中国新通信， 2018,20(09):153\t[60] 王海蓉浅谈大数据时代下国家安全面临的挑战及对策[J]．吉林省经济管理干部学院学\t报， 2016,30(5):5-7.\t[61] 杜燧锋斯诺登事件后，美国情报界有了哪些变化[J]．世界知识， 2016(16):68-69.\t[62] 刘佳讳云计算与大数据环境下的信息安全技术[J]. 电子技术与软件工程， 2019(2):204.\t[63] 吴沈括．数据治理的全球态势及中国应对策略[J]．电子政务， 2019(1):2-10\t[64] 黄道丽，胡文华，大阿来．安全视角下的大数据治理与合规应对[J]．保密科学技术， 2018,\t(10):14-18.\t[65] 孙嘉睿．国内数据治理研究进展：体系、保障与实践［J]. 图书馆学研究， 2018(16):2-8\t[66] 甘似禹，车品觉，杨天顺，吴俊伟大数据治理体系［J]. 计算机应用与软件， 2018,35\t(6): 1-8+69.\t[67] 刘桂锋，钱锦琳，卢章平．国外数据治理模型比较［J]. 图书馆论坛． 2018,38(11):18-26.\t[68] 安小米，郭明军，魏讳，陈慧．大数据治理体系：核心概念、 动议及其实现路径分析[J]．悄\t报资料下作， 2018(1):6-11.\t[69] 刘桂锋，钱锦琳，卢章平．国内外数据治理研究进展：内涵、 要素、 模型与框架［J]. 图\t书情报工作， 2017,61(21):137-144.\t[70] 郑大庆，黄丽华，张成洪，张绍华大数据治理的概念及其参考架构[J]．研究与发展管\t理， 2017,29(4):65-72.\t[71] 唐文剑，吕雯，林松祥，黄浩区块链将如何重新定义世界[M]．机械工业出版社， 2016.\t[72] Melnik, Sergey, et al. \"Dremel: interactive analysis of web-scale datasets. \" Proceedings of\tthe VLDB Endowment 3[J]. 1-2(2010):330-339.\t[73] 陆嘉恒 Hadoop 实战[MJ. 2 版北京：机械工业出版社， 2012.\t[74] Tom White. Hadoop 权威指南（中文版) [M]．周傲英，等，译．北京：清华大学出版社，\t2010.\t[75] 王鹏云计算的关键技术与应用实例[M]. 北京：人民邮电出版社， 2010.\t[76] 黄宜华．深入理解大数据一大数据处理与编程实践[M]．北京：机械工业出版社， 2014.\t[77] 蔡斌，陈湘萍． Hadoop 技术内幕 深入解析 Hadoop Common 和 HDFS 架构设计与实\t现原理[M]．北京：机械工业出版社， 2013.\t266\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 213\n",
      "<class 'str'>\n",
      "： 大数据导论 l\t(2) 在作用层面。 数据分析主要实现三大作用：现状分析、原因分析、预测分析（定址）数据\t分析的目标明确，先做假设，然后通过数据分析来验证假设是否正确，从而得到相应的结论数据挖\t掘主要侧重解决四类问题：分类、聚类、关联和预测（定址、定性）。 数据挖掘的亟点在于寻找术知\t的模式与规律；如著名的数据挖掘案例 啤酒与尿布，就是事先未知的，但又是非常有价值的信息。\t(3) 在方法层面。 数据分析主要采用对比分析、分组分析、交叉分析、回归分析等常用分析\t方法；数据挖掘主要采用决策树、神经网络、关联规则、聚类分析等统计学、人工智能、机器学\t习等方法进行挖掘。\t(4) 在结果层面。 数据分析一般都是得到一个指标统计扯结果，如总和、平均值等，这些数\t据都需要与业务结合进行解读，才能发挥出数据的价值与作用 ） 数据挖掘则是输出模型或规则，\t并且可相应得到模型得分或标签。 模型得分如流失概率值、总和得分、相似度、预测值等，标签\t如高中低价值用户、流失与非流失、信用优良中差等。\t7.1.2 数据分析与数据处理\t数据分析过程通常会伴随着数据处理的发生（或者说伴随着大址数据计算），因此，数据分析\t和数据处理是一对关系紧密的概念，很多时候，二者是融合在一起的，很难割裂开来， 也就是说，\t当用户在进行数据分析的时候，底层的计算机系统会根据数据分析任务的要求，使用程序进行大\t扯的数据处理（或者说发生大批的数据计算）。 例如，当用户进行决策树分析时，需要事先根据决\t策树算法编写分析程序，当分析开始以后，决策树分析程序就会从磁盘读取数据进行大拭计符，\t最终给出计算结果（也就是决策树分析结果）。\t7.1.3 大数据处理与分析\t数据分析包含两个要素，即理论和技术。 在理论层面，需要统计学、机器学习和数据挖掘等\t知识； 在技术层面，包括单机分析工具（如 SPSS、 SAS 等）或单机编程语言（如 Python、 R ),\t以及大数据处理与分析技术（如 MapReduce、 Spark、 Hive 等）。\t数据分析可以是针对小规模数据的分析，也可以是针对大规模数据的分析（这时被称为“大\t数据分析”为在大数据时代到来之前，数据分析主要以小规模的抽样数据为主， 一般使川统计学、\t机器学习和数据挖掘的相关方法，以单机分析工具或者单机编程的方式来实现分析程序。 但是，\t到了大数据时代， 数据址爆炸式地增长，很多时候需要对规模巨大的全蚊数据而不是小规模的抽\t样数据进行分析。 这时，单机下具和单机程序显得“无能为力”，就需要采用分布式实现技术， 比\t如使用 MapReduce、 Spark 或 Flink 编写分布式分析程序，借助于集群的多台机器进行并行数据处\t理分析，这个过程就被称为“大数据处理与分析”。\t本章后续内容中，在数据分析理论层面，只介绍关于数据挖掘的理论知识（即机器学习和数据挖\t掘算法），对于使用统计学方法的狭义的数据分析理论知识（如对比分析、分组分析、交叉分析、预\t测分析、漏斗分析、 A/B 测试分析、结构分析、因素分析、矩阵分析、相关分析、回们分析、聚类分\t析、判断分析、成分分析等）不做介绍，感兴趣的读者可以参考相关的统计学书籍。 在数据分析技术\t202\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 213\n",
      "<class 'str'>\n",
      "： 大数据导论 l\t(2) 在作用层面。 数据分析主要实现三大作用：现状分析、原因分析、预测分析（定址）数据\t分析的目标明确，先做假设，然后通过数据分析来验证假设是否正确，从而得到相应的结论数据挖\t掘主要侧重解决四类问题：分类、聚类、关联和预测（定址、定性）。 数据挖掘的亟点在于寻找术知\t的模式与规律；如著名的数据挖掘案例 啤酒与尿布，就是事先未知的，但又是非常有价值的信息。\t(3) 在方法层面。 数据分析主要采用对比分析、分组分析、交叉分析、回归分析等常用分析\t方法；数据挖掘主要采用决策树、神经网络、关联规则、聚类分析等统计学、人工智能、机器学\t习等方法进行挖掘。\t(4) 在结果层面。 数据分析一般都是得到一个指标统计扯结果，如总和、平均值等，这些数\t据都需要与业务结合进行解读，才能发挥出数据的价值与作用 ） 数据挖掘则是输出模型或规则，\t并且可相应得到模型得分或标签。 模型得分如流失概率值、总和得分、相似度、预测值等，标签\t如高中低价值用户、流失与非流失、信用优良中差等。\t7.1.2 数据分析与数据处理\t数据分析过程通常会伴随着数据处理的发生（或者说伴随着大址数据计算），因此，数据分析\t和数据处理是一对关系紧密的概念，很多时候，二者是融合在一起的，很难割裂开来， 也就是说，\t当用户在进行数据分析的时候，底层的计算机系统会根据数据分析任务的要求，使用程序进行大\t扯的数据处理（或者说发生大批的数据计算）。 例如，当用户进行决策树分析时，需要事先根据决\t策树算法编写分析程序，当分析开始以后，决策树分析程序就会从磁盘读取数据进行大拭计符，\t最终给出计算结果（也就是决策树分析结果）。\t7.1.3 大数据处理与分析\t数据分析包含两个要素，即理论和技术。 在理论层面，需要统计学、机器学习和数据挖掘等\t知识； 在技术层面，包括单机分析工具（如 SPSS、 SAS 等）或单机编程语言（如 Python、 R ),\t以及大数据处理与分析技术（如 MapReduce、 Spark、 Hive 等）。\t数据分析可以是针对小规模数据的分析，也可以是针对大规模数据的分析（这时被称为“大\t数据分析”为在大数据时代到来之前，数据分析主要以小规模的抽样数据为主， 一般使川统计学、\t机器学习和数据挖掘的相关方法，以单机分析工具或者单机编程的方式来实现分析程序。 但是，\t到了大数据时代， 数据址爆炸式地增长，很多时候需要对规模巨大的全蚊数据而不是小规模的抽\t样数据进行分析。 这时，单机下具和单机程序显得“无能为力”，就需要采用分布式实现技术， 比\t如使用 MapReduce、 Spark 或 Flink 编写分布式分析程序，借助于集群的多台机器进行并行数据处\t理分析，这个过程就被称为“大数据处理与分析”。\t本章后续内容中，在数据分析理论层面，只介绍关于数据挖掘的理论知识（即机器学习和数据挖\t掘算法），对于使用统计学方法的狭义的数据分析理论知识（如对比分析、分组分析、交叉分析、预\t测分析、漏斗分析、 A/B 测试分析、结构分析、因素分析、矩阵分析、相关分析、回们分析、聚类分\t析、判断分析、成分分析等）不做介绍，感兴趣的读者可以参考相关的统计学书籍。 在数据分析技术\t202\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 213\n",
      "<class 'str'>\n",
      "： 大数据导论 l\t(2) 在作用层面。 数据分析主要实现三大作用：现状分析、原因分析、预测分析（定址）数据\t分析的目标明确，先做假设，然后通过数据分析来验证假设是否正确，从而得到相应的结论数据挖\t掘主要侧重解决四类问题：分类、聚类、关联和预测（定址、定性）。 数据挖掘的亟点在于寻找术知\t的模式与规律；如著名的数据挖掘案例 啤酒与尿布，就是事先未知的，但又是非常有价值的信息。\t(3) 在方法层面。 数据分析主要采用对比分析、分组分析、交叉分析、回归分析等常用分析\t方法；数据挖掘主要采用决策树、神经网络、关联规则、聚类分析等统计学、人工智能、机器学\t习等方法进行挖掘。\t(4) 在结果层面。 数据分析一般都是得到一个指标统计扯结果，如总和、平均值等，这些数\t据都需要与业务结合进行解读，才能发挥出数据的价值与作用 ） 数据挖掘则是输出模型或规则，\t并且可相应得到模型得分或标签。 模型得分如流失概率值、总和得分、相似度、预测值等，标签\t如高中低价值用户、流失与非流失、信用优良中差等。\t7.1.2 数据分析与数据处理\t数据分析过程通常会伴随着数据处理的发生（或者说伴随着大址数据计算），因此，数据分析\t和数据处理是一对关系紧密的概念，很多时候，二者是融合在一起的，很难割裂开来， 也就是说，\t当用户在进行数据分析的时候，底层的计算机系统会根据数据分析任务的要求，使用程序进行大\t扯的数据处理（或者说发生大批的数据计算）。 例如，当用户进行决策树分析时，需要事先根据决\t策树算法编写分析程序，当分析开始以后，决策树分析程序就会从磁盘读取数据进行大拭计符，\t最终给出计算结果（也就是决策树分析结果）。\t7.1.3 大数据处理与分析\t数据分析包含两个要素，即理论和技术。 在理论层面，需要统计学、机器学习和数据挖掘等\t知识； 在技术层面，包括单机分析工具（如 SPSS、 SAS 等）或单机编程语言（如 Python、 R ),\t以及大数据处理与分析技术（如 MapReduce、 Spark、 Hive 等）。\t数据分析可以是针对小规模数据的分析，也可以是针对大规模数据的分析（这时被称为“大\t数据分析”为在大数据时代到来之前，数据分析主要以小规模的抽样数据为主， 一般使川统计学、\t机器学习和数据挖掘的相关方法，以单机分析工具或者单机编程的方式来实现分析程序。 但是，\t到了大数据时代， 数据址爆炸式地增长，很多时候需要对规模巨大的全蚊数据而不是小规模的抽\t样数据进行分析。 这时，单机下具和单机程序显得“无能为力”，就需要采用分布式实现技术， 比\t如使用 MapReduce、 Spark 或 Flink 编写分布式分析程序，借助于集群的多台机器进行并行数据处\t理分析，这个过程就被称为“大数据处理与分析”。\t本章后续内容中，在数据分析理论层面，只介绍关于数据挖掘的理论知识（即机器学习和数据挖\t掘算法），对于使用统计学方法的狭义的数据分析理论知识（如对比分析、分组分析、交叉分析、预\t测分析、漏斗分析、 A/B 测试分析、结构分析、因素分析、矩阵分析、相关分析、回们分析、聚类分\t析、判断分析、成分分析等）不做介绍，感兴趣的读者可以参考相关的统计学书籍。 在数据分析技术\t202\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 213\n",
      "<class 'str'>\n",
      "： 大数据导论 l\t(2) 在作用层面。 数据分析主要实现三大作用：现状分析、原因分析、预测分析（定址）数据\t分析的目标明确，先做假设，然后通过数据分析来验证假设是否正确，从而得到相应的结论数据挖\t掘主要侧重解决四类问题：分类、聚类、关联和预测（定址、定性）。 数据挖掘的亟点在于寻找术知\t的模式与规律；如著名的数据挖掘案例 啤酒与尿布，就是事先未知的，但又是非常有价值的信息。\t(3) 在方法层面。 数据分析主要采用对比分析、分组分析、交叉分析、回归分析等常用分析\t方法；数据挖掘主要采用决策树、神经网络、关联规则、聚类分析等统计学、人工智能、机器学\t习等方法进行挖掘。\t(4) 在结果层面。 数据分析一般都是得到一个指标统计扯结果，如总和、平均值等，这些数\t据都需要与业务结合进行解读，才能发挥出数据的价值与作用 ） 数据挖掘则是输出模型或规则，\t并且可相应得到模型得分或标签。 模型得分如流失概率值、总和得分、相似度、预测值等，标签\t如高中低价值用户、流失与非流失、信用优良中差等。\t7.1.2 数据分析与数据处理\t数据分析过程通常会伴随着数据处理的发生（或者说伴随着大址数据计算），因此，数据分析\t和数据处理是一对关系紧密的概念，很多时候，二者是融合在一起的，很难割裂开来， 也就是说，\t当用户在进行数据分析的时候，底层的计算机系统会根据数据分析任务的要求，使用程序进行大\t扯的数据处理（或者说发生大批的数据计算）。 例如，当用户进行决策树分析时，需要事先根据决\t策树算法编写分析程序，当分析开始以后，决策树分析程序就会从磁盘读取数据进行大拭计符，\t最终给出计算结果（也就是决策树分析结果）。\t7.1.3 大数据处理与分析\t数据分析包含两个要素，即理论和技术。 在理论层面，需要统计学、机器学习和数据挖掘等\t知识； 在技术层面，包括单机分析工具（如 SPSS、 SAS 等）或单机编程语言（如 Python、 R ),\t以及大数据处理与分析技术（如 MapReduce、 Spark、 Hive 等）。\t数据分析可以是针对小规模数据的分析，也可以是针对大规模数据的分析（这时被称为“大\t数据分析”为在大数据时代到来之前，数据分析主要以小规模的抽样数据为主， 一般使川统计学、\t机器学习和数据挖掘的相关方法，以单机分析工具或者单机编程的方式来实现分析程序。 但是，\t到了大数据时代， 数据址爆炸式地增长，很多时候需要对规模巨大的全蚊数据而不是小规模的抽\t样数据进行分析。 这时，单机下具和单机程序显得“无能为力”，就需要采用分布式实现技术， 比\t如使用 MapReduce、 Spark 或 Flink 编写分布式分析程序，借助于集群的多台机器进行并行数据处\t理分析，这个过程就被称为“大数据处理与分析”。\t本章后续内容中，在数据分析理论层面，只介绍关于数据挖掘的理论知识（即机器学习和数据挖\t掘算法），对于使用统计学方法的狭义的数据分析理论知识（如对比分析、分组分析、交叉分析、预\t测分析、漏斗分析、 A/B 测试分析、结构分析、因素分析、矩阵分析、相关分析、回们分析、聚类分\t析、判断分析、成分分析等）不做介绍，感兴趣的读者可以参考相关的统计学书籍。 在数据分析技术\t202\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 130\n",
      "<class 'str'>\n",
      "l 第 4章大数据应用仁＝：~\t推荐系统通过发掘用户的历史记录，找到用户的个性化需求，发现用户潜在的消费倾向，从\t而将长尾商品准确地推荐给可能需要它的用户，帮助用户发现那些他们感兴趣却很难发现的商品，\t最终实现用户与商家双赢。\t4.1.3 推荐方法\t推荐系统的本质是建立用户与商品的联系，根据推荐算法的不同，推荐方法包括以下几类。\t(1) 专家推荐。 专家推荐是传统的推荐方式，本质上是一种人工推荐，由资深的专业人士来\t进行商品的筛选和推荐，这需要较多的人力成本。 现在专家推荐结果主要作为其他推荐算法结果\t的补充。\t(2) 基于统计的推荐。 基于统计信息的推荐（如热门推荐），概念直观，易于实现，但是，对\t用户个性化偏好的描述能力较弱。\t(3) 基于内容的推荐。 基于内容的推荐是信息过滤技术的延续与发展，其更多是通过机器学\t习的方法去描述内容的特征，并基于内容的特征来发现与之相似的内容。\t(4) 协同过滤推荐。 协同过滤推荐是推荐系统中应用最早和最为成功的技术之一。 它一般采\t用最近邻技术，利用用户的历史信息计算用户之间的距离，然后利用目标用户的最近邻居用户对\t商品的评价信息，来预测目标用户对特定商品的喜好程度，最后根据这一喜好程度来对目标用户\t进行推荐。\t(5) 混合推荐。 在实际应用中，单一的推荐算法往往无法取得良好的推荐效果，因此，多数\t推荐系统会对多种推荐算法进行有机组合，如在协同过滤之上加入基于内容的推荐。\t基于内容的推荐与协同过滤推荐有相似之处，但是，基于内容的推荐关注的是商品本身的特\t征，通过商品自身的特征来找到相似的商品；协同过滤推荐则依赖用户与商品间的联系，与商品\t自身特征没有太多关系。\t4.1.4 推荐系统模型\t推荐系统基本架构如图 4-2 所示。一个完整的推荐系统通常包括 3 个组成模块：用户建模模\t块、推荐对象建模模块、推荐算法模块。 推荐系统首先对用户进行建模，根据用户行为数据和属\t性数据来分析用户的兴趣和需求，同时对推荐对象进行建模。 接着，基于用户特征和商品特征，\t采用推荐算法计算得到用户可能感兴趣的对象，之后根据推荐场景对推荐结果进行一定程度的过\t滤和调整，最终将推荐结果展示给用户。\t推荐系统通常需要处理庞大的数据，既要考虑推荐的准确度，也要考虑计算推荐结果所需的\t时间，因此，推荐系统一般可再细分成离线计算部分与实时计算部分。 离线计算部分对于数据量、\t算法复杂度、时间限制均较少，可得出较高准确度的推荐结果。 在线计算部分则要求能快速响应\t推荐请求，能容忍相对较低的推荐准确度。 通过将实时推荐结果与离线推荐结果相结合，推荐系\t统能为用户提供高质批的推荐结果。\t119\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 217\n",
      "<class 'str'>\n",
      "二＝＝」大数据导论 I\t(2) 对这些关系式的可信程度进行检验。\t(3) 在许多自变量共同影响着一个因变量的关系中，判断哪个（或哪些）自变最的影响是显\t著的，哪个（或哪些）自变批的影响是不显著的，将影响显著的自变址加入模型中，剔除影响不\t显著的变批，通常用逐步回归、向前回归和向后回归等方法。\t(4) 利用所求的关系式对某一生产过程进行预测或控制。\t7.2.5 关联规则\t关联规则最初是针对购物篮分析 (Market Basket Analysis) 问题提出的。 假设零售商想更多\t地了解顾客的购物习惯，比如想知道顾客可能会在一次购物时同时购买哪些商品。 为了同答该问\t题，可以对商店的顾客购物数据进行购物篮分析。 该过程通过发现顾客放入“购物篮”中的不同\t商品之间的关联，分析顾客的购物习惯。 这种关联的发现可以帮助零售商了解哪些商品频繁地被\t顾客购买，从而帮助他们制订更好的营销策略。\t关联规则定义为：假设 1={11,I凶，···Jm}是项的集合。 给定一个交易数据库 D, 其中每个事务\tt 是 I 的非空子集，即每一个交易都与一个唯一的标识符 TID 对应。 关联规则在 D 中的支持度是\tD 中事务同时包含 X、 Y的百分比，即概率；置信度是在 D 中事务已经包含X的情况下，包含 Y\t的百分比，即条件概率。 如果满足最小支待度闾值和最小置信度阔值，则认为关联规则是可信的。\t这些阙值是根据挖掘需要人为设定的。\t这里举一个简单的例子进行说明。表7-1 是数据库D 中的顾客购买记录，包含6个事务。项集I=｛乒\t乓球拍，乒乓球，运动鞋，羽毛球｝。 考虑关联规则（频繁二项集）：乒乓球拍与乒乓球，事务 l、 2、\t3、 4、 6包含乒乓球拍，事务 l 、 2、 6 同时包含乒乓球拍和乒乓球，这里用X表示购买了乒乓球，用\tY表示购买了乒乓球拍，则X/\\Y=3, D=6, 支持度(X/\\Y)ID书．5; X=5，置信度(X/\\Y)/X=O.6。 若给定\t最小支持度 a=0.5, 最小置信度/3=0.6, 则认为购买乒乓球拍和购买乒乓球之间存在关联。\t表7-1 顾客购买记录\tTID 乒乓球拍 乒乓球 运动鞋 羽毛球\tl I I l 。\t2 l 1 。 。\t3 l 。 。 。\t4 I 。 I 。\t5 。 1 1 I\t6 I 1 。 。\t常见的关联规则挖掘算法包括 Apriori 算法和 FP-Growth 算法等。\t7.2.6 协同过滤\t推荐技术从被提出到现在已有十余年，在多年的发展历程中诞生了很多新的推荐算法。 协同\t过滤作为最早、 最知名的推荐算法，不仅在学术界得到了深入研究，而且至今在业界仍有广泛的\t应用，已经被大量应用到电子商务的推荐系统中。 协同过滤主要包括基千用户的协同过滤、 基于\t206\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "idx: 180\n",
      "<class 'str'>\n",
      "l 第 5 章数据采集与预处理 仁．~\t(4) 规范化处理：将属性值按比例缩放，使之落入一个特定的区间，比如 0.0......,1.0。 常用的\t数据规范化方法包括 Min-Max 规范化、 Z-Score规范化和小数定标规范化等。\t(5) 属性构造处理：根据已有属性集构造新的属性，后续数据处理直接使用新增的属性。 例\t如，根据已知的质批和体积屈性，计算出新的属性——密度。\t5.3.2 平滑处理\t噪声是指被测变批的一个随机错误和变化。 平滑处理旨在帮助去掉数据中的噪声。 常用的方\t法包括分箱、回归和聚类等。\t1．分箱\t分箱 ( Bin) 利用被平滑数据点的周围点（近邻点），对一组排序数据进行平滑处理，排序后\t的数据被分配到若干箱子中。\t典型的分箱方法一般有两种： 一种是等高方法，即每个箱子中元素的个数相等；另一种是等\t宽方法，即每个箱子的取值间距（左右边界之差）相同，如图 5-6 所示。\t箱子中元素的个数 箱子中元素的个数\t(a)等高方法 属性值 (b)等宽方法 属性值\t图 5-6 两种典型分箱方法\t这里给出一个实例介绍分箱。 假设有一个数据集X={4,8,15,21,21,24,25,28,34} ，这里采用基于\t平均值的等高方法对其进行平滑处理，则分箱处理的步骤如下。\t(1) 把原始数据集 X放入以下 3 个箱子。\t箱子 1: 4,8,15。\t箱子 2: 21,21,24。\t箱子 3: 25,28,34。\t(2) 分别计算每个箱子的平均值。\t箱子 1 的平均值： 9。\t箱子 2 的平均值： 22。\t箱子 3 的平均值： 29。\t(3) 用每个箱子的平均值替换该箱子内的所有元素。\t箱子 1 : 9,9,9。\t箱子 2: 22,22,22。\t箱子 3: 29,29,29。\t(4) 合并各个箱子中的元素得到新的数据集{9,9,9,22,22,22,29,29必9}。\t169\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reference_list = [x['reference'].split('_')[1] for x in questions]\n",
    "reference_list\n",
    "# pdf_content['page_'+reference_list[0]].replace('\\n','\\t')\n",
    "\n",
    "for idx in reference_list:\n",
    "    print(\"idx:\",idx)\n",
    "    print(type(idx))\n",
    "    print(pdf_content['page_' + idx].replace('\\n', '\\t'))\n",
    "    print('-------------------'*10)\n",
    "# for i in questions:\n",
    "#     print(i[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:38:01.639718600Z",
     "start_time": "2024-05-26T09:38:01.606723Z"
    }
   },
   "id": "413bd5c772fb297c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 0\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 1\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 2\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 3\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 4\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 5\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 6\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 7\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 8\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 9\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 10\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 11\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 12\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 13\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 14\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 15\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 16\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 17\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 18\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 19\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 20\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 21\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 22\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 23\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 24\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 25\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 26\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 27\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 28\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 29\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 30\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 31\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 32\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 33\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 34\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 35\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 36\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 37\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 38\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 39\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 40\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 41\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 42\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 43\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 44\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 45\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 46\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 47\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 48\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 49\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 50\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 51\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 52\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 53\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 54\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 55\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 56\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 57\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 58\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 59\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(questions)):\n",
    "\n",
    "    prompt = '''你是一个大数据教师，帮我结合给定的资料和选项，请先判断题目的类型，再以教师的身份回答以：题目类型： 答案： 解析： 的形式回答问题。\n",
    "    资料：{0}\n",
    "\n",
    "    问题：{1}\n",
    "    \n",
    "    选项：{2}\n",
    "        '''.format(\n",
    "        pdf_content['page_'+reference_list[idx]].replace('\\n','\\t'),\n",
    "        questions[idx][\"question\"],\n",
    "        questions[idx][\"choice\"]\n",
    "    )\n",
    "    # print(\"idx:\",idx)\n",
    "    # print(\"prompt:\",prompt)\n",
    "    answer = glm_predict(prompt)['choices'][0]['message']['content']\n",
    "    questions[idx]['answers'] = answer\n",
    "    #追加写入answer.json文件\n",
    "    print(\"idx:\",idx)\n",
    "with open('./大数据导论知识问答/answers_llms_prompt_optimize2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, ensure_ascii=False, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T09:51:11.973429600Z",
     "start_time": "2024-05-26T09:46:44.436113500Z"
    }
   },
   "id": "a01de4e91598e5dc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 0\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 1\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 2\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 3\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 4\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 5\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 6\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 7\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 8\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 9\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 10\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 11\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 12\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 13\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 14\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 15\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 16\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 17\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 18\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 19\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 20\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 21\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 22\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 23\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 24\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 25\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 26\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 27\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 28\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 29\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 30\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 31\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 32\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 33\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 34\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 35\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 36\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 37\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 38\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 39\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 40\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 41\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 42\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 43\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 44\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 45\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 46\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 47\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 48\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 49\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 50\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 51\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 52\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 53\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 54\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 55\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 56\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 57\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 58\n",
      "LLM Response:\n",
      "<Response [200]>\n",
      "idx: 59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in range(len(questions)):\n",
    "\n",
    "    prompt = '''你是一个大数据教师，帮我结合给定的资料和选项，请先判断题目的类型，再以教师的身份回答以\"题目类型\"： ,\"答案\"：注：只需要答案，不要选项内容！ ,\"解析\"：的json形式回答问题。\n",
    "    资料：{0}\n",
    "\n",
    "    问题：{1}\n",
    "    \n",
    "    选项：{2}\n",
    "        '''.format(\n",
    "        pdf_content['page_'+reference_list[idx]].replace('\\n','\\t'),\n",
    "        questions[idx][\"question\"],\n",
    "        questions[idx][\"choice\"]\n",
    "    )\n",
    "    # print(\"idx:\",idx)\n",
    "    # print(\"prompt:\",prompt)\n",
    "    answer = glm_predict(prompt)['choices'][0]['message']['content']\n",
    "    questions[idx]['answers'] = answer\n",
    "    #追加写入answer.json文件\n",
    "    print(\"idx:\",idx)\n",
    "with open('./大数据导论知识问答/answers_llms_prompt_optimize3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, ensure_ascii=False, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T10:18:04.288119200Z",
     "start_time": "2024-05-26T10:14:34.607560900Z"
    }
   },
   "id": "1bb5389bc97faa5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "54723c732cb2f78e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
